<!doctype html>
<html >
<head>
    
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <!--[if lt IE 9]>
                <script src="http://css3-mediaqueries-js.googlecode.com/svn/trunk/css3-mediaqueries.js"></script>
        <![endif]-->
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta http-equiv="Content-Style-Type" content="text/css" />

    <!-- <link rel="stylesheet" type="text/css" href="template.css" /> -->
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/template.css" />

    <link href="https://vjs.zencdn.net/5.4.4/video-js.css" rel="stylesheet" />

    <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
    <!-- <script type='text/javascript' src='menu/js/jquery.cookie.js'></script> -->
    <!-- <script type='text/javascript' src='menu/js/jquery.hoverIntent.minified.js'></script> -->
    <!-- <script type='text/javascript' src='menu/js/jquery.dcjqaccordion.2.7.min.js'></script> -->

    <!-- <link href="menu/css/skins/blue.css" rel="stylesheet" type="text/css" /> -->
    <!-- <link href="menu/css/skins/graphite.css" rel="stylesheet" type="text/css" /> -->
    <!-- <link href="menu/css/skins/grey.css" rel="stylesheet" type="text/css" /> -->
  
    <!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
        
  
    <!-- <script src="script.js"></script> -->
  
    <!-- <script src="jquery.sticky-kit.js "></script> -->
    <script type='text/javascript' src='https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/js/jquery.cookie.js'></script>
    <script type='text/javascript' src='https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/js/jquery.hoverIntent.minified.js'></script>
    <script type='text/javascript' src='https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/js/jquery.dcjqaccordion.2.7.min.js'></script>

    <link href="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/css/skins/blue.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/css/skins/graphite.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/css/skins/grey.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.jsdelivr.net/gh/ryangrose/easy-pandoc-templates@948e28e5/css/elegant_bootstrap.css" rel="stylesheet" type="text/css" />
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
    <script src="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/script.js"></script>
  
    <script src="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/jquery.sticky-kit.js"></script>
    <meta name="generator" content="pandoc" />
  <title>Machine Learning summary</title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>

    
    <div class="navbar navbar-static-top">
    <div class="navbar-inner">
      <div class="container">
        <span class="doc-title">Machine Learning summary</span>
        <ul class="nav pull-right doc-info">
                            </ul>
      </div>
    </div>
  </div>
    <div class="container">
    <div class="row">
            <div id="TOC" class="span3">
        <div class="well toc">

        <ul>
        <li><a href="#machine-learning-summary"
        id="toc-machine-learning-summary">Machine Learning summary</a>
        <ul>
        <li><a href="#the-ml-landscape" id="toc-the-ml-landscape">1. The
        ML landscape</a>
        <ul>
        <li><a href="#ml-types" id="toc-ml-types">ML types</a></li>
        <li><a href="#model-based-vs-instance-based-learning"
        id="toc-model-based-vs-instance-based-learning">Model based vs
        instance based learning</a></li>
        <li><a href="#training-a-model"
        id="toc-training-a-model">Training a model</a></li>
        <li><a href="#ml-workflow" id="toc-ml-workflow">ML
        workflow</a></li>
        <li><a href="#problems-with-bad-datasets"
        id="toc-problems-with-bad-datasets">Problems with bad
        datasets</a></li>
        </ul></li>
        <li><a href="#end-to-end-ml-project"
        id="toc-end-to-end-ml-project">2. End-to-end ML project</a>
        <ul>
        <li><a href="#workflow" id="toc-workflow">Workflow</a></li>
        <li><a href="#exploratory-data-analysis-eda"
        id="toc-exploratory-data-analysis-eda">Exploratory Data Analysis
        (EDA)</a></li>
        </ul></li>
        <li><a href="#classification" id="toc-classification">3.
        Classification</a>
        <ul>
        <li><a href="#performance-metrics"
        id="toc-performance-metrics">Performance Metrics</a></li>
        <li><a href="#binary-classification"
        id="toc-binary-classification">Binary Classification</a></li>
        <li><a href="#multiclass-classification"
        id="toc-multiclass-classification">Multiclass
        classification</a></li>
        </ul></li>
        <li><a href="#training-models" id="toc-training-models">4.
        Training models</a>
        <ul>
        <li><a href="#linear-regression"
        id="toc-linear-regression">Linear regression</a></li>
        <li><a href="#polynomial-regression"
        id="toc-polynomial-regression">Polynomial regression</a></li>
        <li><a href="#model-regularization"
        id="toc-model-regularization">Model regularization</a></li>
        <li><a href="#cross-validation" id="toc-cross-validation">Cross
        validation</a></li>
        <li><a href="#hyper-parameter-optimization"
        id="toc-hyper-parameter-optimization">Hyper parameter
        optimization</a></li>
        <li><a href="#classification-1"
        id="toc-classification-1">Classification</a></li>
        </ul></li>
        <li><a href="#super-vector-machines-linear-classification"
        id="toc-super-vector-machines-linear-classification">5. Super
        Vector Machines: Linear classification</a>
        <ul>
        <li><a href="#largest-margin-classifier"
        id="toc-largest-margin-classifier">Largest margin
        classifier</a></li>
        <li><a href="#soft-margin-classification"
        id="toc-soft-margin-classification">Soft margin
        classification</a></li>
        <li><a href="#hard-margin-vs-soft-margin"
        id="toc-hard-margin-vs-soft-margin">Hard margin vs soft
        margin</a></li>
        <li><a href="#solving-svms" id="toc-solving-svms">Solving
        SVM’s</a></li>
        <li><a href="#svm-for-non-linear-classification"
        id="toc-svm-for-non-linear-classification">SVM for non-linear
        classification</a></li>
        </ul></li>
        <li><a href="#decision-trees" id="toc-decision-trees">6.
        Decision Trees</a>
        <ul>
        <li><a href="#decision-trees-for-classification-vs-regression"
        id="toc-decision-trees-for-classification-vs-regression">Decision
        trees for classification vs regression</a></li>
        <li><a href="#building-a-decision-tree"
        id="toc-building-a-decision-tree">Building a decision
        tree</a></li>
        <li><a
        href="#cart-classification-and-regression-trees-algorithm"
        id="toc-cart-classification-and-regression-trees-algorithm">CART
        (classification and regression trees) algorithm</a></li>
        <li><a href="#regularization"
        id="toc-regularization">Regularization</a></li>
        <li><a href="#are-decision-trees-any-good"
        id="toc-are-decision-trees-any-good">Are decision trees any
        good?</a></li>
        </ul></li>
        <li><a href="#ensembles" id="toc-ensembles">7. Ensembles</a>
        <ul>
        <li><a href="#voting" id="toc-voting">Voting</a></li>
        <li><a href="#boosting" id="toc-boosting">Boosting</a></li>
        <li><a href="#stacking" id="toc-stacking">Stacking</a></li>
        </ul></li>
        <li><a href="#dimensionality-reduction"
        id="toc-dimensionality-reduction">8. Dimensionality
        Reduction</a>
        <ul>
        <li><a href="#curse-of-dimensionality"
        id="toc-curse-of-dimensionality">Curse of
        dimensionality</a></li>
        <li><a href="#linear-dimensionality-reduction"
        id="toc-linear-dimensionality-reduction">Linear dimensionality
        reduction</a></li>
        <li><a href="#non-linear-dimensionality-reduction"
        id="toc-non-linear-dimensionality-reduction">Non-linear
        dimensionality reduction</a></li>
        </ul></li>
        <li><a href="#unsupervised-learning"
        id="toc-unsupervised-learning">9. Unsupervised learning</a>
        <ul>
        <li><a href="#distance-based-algo"
        id="toc-distance-based-algo">Distance-based algo</a></li>
        <li><a
        href="#probability-density-based-density-estimation-gaussian-mixture-model"
        id="toc-probability-density-based-density-estimation-gaussian-mixture-model">Probability-density
        based (density estimation): Gaussian Mixture Model</a></li>
        <li><a href="#density-estimation-maximum-likelihood"
        id="toc-density-estimation-maximum-likelihood">Density
        estimation: Maximum likelihood</a></li>
        <li><a href="#soft-clustering-gaussian-mixture-distribution"
        id="toc-soft-clustering-gaussian-mixture-distribution">Soft
        clustering: Gaussian Mixture Distribution</a></li>
        <li><a href="#multivariate-gmm"
        id="toc-multivariate-gmm">multivariate GMM</a></li>
        </ul></li>
        <li><a
        href="#introduction-to-artificial-neural-networks-with-keras"
        id="toc-introduction-to-artificial-neural-networks-with-keras">10.
        Introduction to Artificial Neural Networks with Keras</a>
        <ul>
        <li><a href="#wave-1-perceptron" id="toc-wave-1-perceptron">10.1
        Wave 1: Perceptron</a></li>
        <li><a href="#wave-2-distributed-representations"
        id="toc-wave-2-distributed-representations">10.2 Wave 2:
        Distributed representations</a></li>
        <li><a href="#wave-3-deep-learning"
        id="toc-wave-3-deep-learning">10.3 Wave 3: Deep
        learning</a></li>
        </ul></li>
        <li><a href="#training-deep-neural-networks"
        id="toc-training-deep-neural-networks">11. Training Deep Neural
        Networks</a>
        <ul>
        <li><a href="#vanishing-exploding-gradients"
        id="toc-vanishing-exploding-gradients">Vanishing / exploding
        gradients</a></li>
        <li><a href="#optimization-algorithms"
        id="toc-optimization-algorithms">Optimization
        algorithms</a></li>
        <li><a href="#learning-rate-scheduling"
        id="toc-learning-rate-scheduling">Learning rate
        scheduling</a></li>
        <li><a href="#regularization-1"
        id="toc-regularization-1">Regularization</a></li>
        <li><a href="#normalization"
        id="toc-normalization">Normalization</a></li>
        <li><a href="#hyper-parameter-tuning"
        id="toc-hyper-parameter-tuning">Hyper parameter tuning</a></li>
        </ul></li>
        <li><a
        href="#deep-computer-vision-using-convolutional-neural-networks"
        id="toc-deep-computer-vision-using-convolutional-neural-networks">14.
        Deep Computer vision using convolutional neural networks</a>
        <ul>
        <li><a href="#convolutional-neural-networks"
        id="toc-convolutional-neural-networks">Convolutional neural
        networks</a></li>
        <li><a href="#pooling" id="toc-pooling">Pooling</a></li>
        <li><a href="#cnn-architecture" id="toc-cnn-architecture">CNN
        architecture</a></li>
        </ul></li>
        <li><a href="#processing-sequences-using-rnns-and-cnns"
        id="toc-processing-sequences-using-rnns-and-cnns">15. Processing
        Sequences using RNNs and CNNs</a>
        <ul>
        <li><a href="#rnn-architectures" id="toc-rnn-architectures">RNN
        architectures</a></li>
        <li><a href="#backpropagation-through-time"
        id="toc-backpropagation-through-time">Backpropagation through
        time</a></li>
        <li><a href="#long-short-term-memory-lstms"
        id="toc-long-short-term-memory-lstms">Long-Short Term Memory
        (LSTMs)</a></li>
        <li><a href="#gated-recurrent-units-gru"
        id="toc-gated-recurrent-units-gru">Gated Recurrent Units
        (GRU)</a></li>
        </ul></li>
        <li><a
        href="#natural-language-processing-with-rnns-and-attention-transformers"
        id="toc-natural-language-processing-with-rnns-and-attention-transformers">16.
        Natural Language processing with RNNs and attention
        (<strong>Transformers</strong>)</a>
        <ul>
        <li><a href="#natural-language-preprocessing-nlp"
        id="toc-natural-language-preprocessing-nlp">Natural Language
        Preprocessing (NLP)</a></li>
        <li><a href="#rnns-for-text-generation-seq2seq"
        id="toc-rnns-for-text-generation-seq2seq">RNNs for text
        generation (seq2seq)</a></li>
        <li><a href="#rnns-for-text-classification-seq2vec"
        id="toc-rnns-for-text-classification-seq2vec">RNNs for text
        classification (seq2vec)</a></li>
        <li><a href="#neural-machine-translation-encoder-decoder"
        id="toc-neural-machine-translation-encoder-decoder">Neural
        machine translation (encoder-decoder)</a></li>
        <li><a href="#attention-mechanisms"
        id="toc-attention-mechanisms">Attention mechanisms</a></li>
        <li><a href="#transformers"
        id="toc-transformers">Transformers</a></li>
        <li><a href="#gpt" id="toc-gpt">GPT</a></li>
        <li><a href="#vision-transformer"
        id="toc-vision-transformer">Vision transformer</a></li>
        </ul></li>
        <li><a href="#autoencoders-gans-and-diffusion-models"
        id="toc-autoencoders-gans-and-diffusion-models">17.
        Autoencoders, GANs and diffusion models</a>
        <ul>
        <li><a href="#representation-learning-and-generative-modelling"
        id="toc-representation-learning-and-generative-modelling">Representation
        learning and generative modelling</a></li>
        <li><a href="#autoencoders"
        id="toc-autoencoders">Autoencoders</a></li>
        <li><a href="#generative-adversarial-models-gan"
        id="toc-generative-adversarial-models-gan">Generative
        adversarial models (GAN)</a></li>
        <li><a href="#diffusion-models"
        id="toc-diffusion-models">Diffusion models</a></li>
        <li><a href="#stable-diffusion" id="toc-stable-diffusion">Stable
        diffusion</a></li>
        </ul></li>
        </ul></li>
        </ul>

        </div>
      </div>
            <div class="span9">

      
      <h1 id="machine-learning-summary">Machine Learning summary</h1>
<h2 id="the-ml-landscape">1. The ML landscape</h2>
<h3 id="ml-types">ML types</h3>
<table>
<colgroup>
<col style="width: 41%" />
<col style="width: 32%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th>Supervised</th>
<th>Unsupervised</th>
<th>Reinforcement</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Labeled data</td>
<td>No labels</td>
<td>Decision process</td>
</tr>
<tr class="even">
<td>Direct feedback</td>
<td>No feedback</td>
<td>Reward system</td>
</tr>
<tr class="odd">
<td>Predict outcome/future</td>
<td>Find hidden structures in data</td>
<td>Learn series of actions</td>
</tr>
<tr class="even">
<td>eg: Classification or regression<a href="#fn1" class="footnote-ref"
id="fnref1" role="doc-noteref"><sup>1</sup></a></td>
<td>eg: Anomaly detection</td>
<td>eg: alphago</td>
</tr>
</tbody>
</table>
<h3 id="model-based-vs-instance-based-learning">Model based vs instance
based learning</h3>
<table>
<colgroup>
<col style="width: 45%" />
<col style="width: 54%" />
</colgroup>
<thead>
<tr class="header">
<th>Model-based</th>
<th>Instance-based</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Evaluate a mathematical function on the unseen instance</td>
<td>Measure similarities between unseen instance and training
instance</td>
</tr>
</tbody>
</table>
<h3 id="training-a-model">Training a model</h3>
<ol type="1">
<li>Choose a parameterized model family (<span class="math inline">life
satisfaction = <em>θ</em><sub>0</sub> + <em>θ</em><sub>1</sub> ⋅ GDP_per_capita</span>)</li>
<li>Find parameter values that maximize a fitness function or minimize a
cost function</li>
</ol>
<dl>
<dt><strong>No free lunch</strong></dt>
<dd>
There is no model that is guaranteed to work better, a lot of testing
must happen to choose and fine-tune the model.
</dd>
</dl>
<h4 id="testing-and-validation">Testing and validation</h4>
<figure>
<img src="./img/testing_train_test.png"
alt="train using train and test data" />
<figcaption aria-hidden="true">train using train and test
data</figcaption>
</figure>
<figure>
<img src="./img/testing_train_test_val_png"
alt="train using train, test and validation data" />
<figcaption aria-hidden="true">train using train, test and validation
data</figcaption>
</figure>
<h4 id="overfitting">Overfitting</h4>
<p>Model doesn’t generalize enough. It learns your specific training
data but underperforms on new data.</p>
<p>Possible cures:</p>
<ul>
<li>Use a bigger dataset</li>
<li>Simplify model</li>
<li>Reduce the noice in the dataset</li>
</ul>
<h4 id="underfitting">Underfitting</h4>
<p>When the performance is even bad on the training data</p>
<p>Possible cures:</p>
<ul>
<li>Increase number of parameters</li>
<li>Add more features</li>
<li>Reduce the regularization parameters</li>
</ul>
<h3 id="ml-workflow">ML workflow</h3>
<figure>
<img src="./img/ml_workflow.png" alt="Machine learning workflow" />
<figcaption aria-hidden="true">Machine learning workflow</figcaption>
</figure>
<h3 id="problems-with-bad-datasets">Problems with bad datasets</h3>
<p><strong>Sampling bias</strong>: Dataset can be non-representative if
it has an underrepresented class.</p>
<p>Garbage in == Garbage out: Bad dataset is guaranteed to lead to a bad
(trained) model.</p>
<h2 id="end-to-end-ml-project">2. End-to-end ML project</h2>
<h3 id="workflow">Workflow</h3>
<figure>
<img src="./img/project_workflow.png" alt="Project workflow" />
<figcaption aria-hidden="true">Project workflow</figcaption>
</figure>
<h3 id="exploratory-data-analysis-eda">Exploratory Data Analysis
(EDA)</h3>
<ol type="1">
<li>Get an initial feel of the data</li>
<li>Visualize and gain insight</li>
<li>Prepare the data</li>
</ol>
<h4 id="what-to-do-with-missing-values">What to do with missing
values</h4>
<ul>
<li>Remove entry</li>
<li><strong>Imputation</strong>: replace by mean, median, 0, …</li>
</ul>
<h4 id="categorical-attributes">Categorical attributes</h4>
<p>Attributes that can only take a limited number of values (eg: T-shirt
sizes)</p>
<p><strong>one-hot-encoding</strong>: Use on categorical variables to
transform them into a format the model can understand.</p>
<h4 id="features-scaling">Features scaling</h4>
<p>Some extremely big or small features may have an abnormally large
impact on the model. This can be solved by <em>rescaling</em> them using
the following techniques:</p>
<ul>
<li><strong>Normalization</strong> (min-max scaling): <span
class="math inline">$x_{norm} = \frac{x - \min(x)}{\max(x) -
\min(x)}$</span></li>
<li><strong>Standardization</strong>: <span
class="math inline">$x_{stand} = \frac{x -
mean(x)}{\text{standarddeviation}(x)}$</span></li>
</ul>
<h2 id="classification">3. Classification</h2>
<p>Classification always happens by <em>supervised learning</em>
models</p>
<h3 id="performance-metrics">Performance Metrics</h3>
<h4 id="accuracy">Accuracy</h4>
<dl>
<dt><strong>Accuracy</strong></dt>
<dd>
The percentage of predicted labels that corresponds with the ground
truth label.
</dd>
</dl>
<p><span class="math inline">$Accuracy = \frac{TP +
TN}{Total}$</span></p>
<h4 id="confusion-matrix">Confusion Matrix</h4>
<p>Columns are predicted labels, rows are true labels</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Automobile</th>
<th>No Automobile</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Automobile</strong></td>
<td>True Positive</td>
<td>False Negative</td>
</tr>
<tr class="even">
<td><strong>No Automobile</strong></td>
<td>False Positive</td>
<td>True Negative</td>
</tr>
</tbody>
</table>
<h4 id="precision-and-recall">Precision and recall</h4>
<dl>
<dt><strong>Precision</strong></dt>
<dd>
Accuracy of the positive predictions: <span
class="math inline">$precision = \frac{TP}{TP+FP}$</span>
</dd>
<dt>Recall</dt>
<dd>
How many of the actual positives are detected?: <span
class="math inline">$recall = \frac{TP}{TP+FN}$</span>
</dd>
</dl>
<p><strong>When do we want high precision?</strong></p>
<ul>
<li>When false positives are costly (eg: medical predictions, fraud
detection). You really don’t want falsely flag a condition.</li>
<li>In imbalanced datasets where the majority class is the negative
one.</li>
</ul>
<p><strong>When do we want high accuracy?</strong></p>
<ul>
<li>False negatives are costly (eg: cancer detection or nsfw
filters)</li>
<li>Information retrieval: recall helps ensure that all relevant
documents or information are retrieved</li>
</ul>
<h4 id="f1-score">F1 score</h4>
<p>It combines the precision and recall of a model into a single
metric.</p>
<p><span class="math inline">$F_{1} = 2 \cdot \frac{precision \cdot
recall}{precision + recall}$</span></p>
<h3 id="binary-classification">Binary Classification</h3>
<dl>
<dt><strong>Binary</strong></dt>
<dd>
Only two classes.
</dd>
</dl>
<ul>
<li><strong>Decision boundary</strong>: hypersurface that partitions
underlying vector space into two sets, one for each class.</li>
<li><strong>Score/Class Probability</strong>: <span
class="math inline">$\hat{y}(x^{(i)}) = \begin{cases} +1 \quad
\text{if}\quad h_{\theta}(x^{(i)}) \geq T \\ -1 \quad \text{if}\quad
h_{\theta}(x^{(i)}) &lt; T \end{cases}$</span> (T = threshold as
hyperparam)</li>
</ul>
<h4 id="choosing-a-threshold">Choosing a threshold</h4>
<h5 id="precision-vs-recall-choosing-a-threshold">Precision vs Recall
(Choosing a threshold)</h5>
<figure>
<img src="./img/precision_vs_recall.png" alt="Precision vs recall" />
<figcaption aria-hidden="true">Precision vs recall</figcaption>
</figure>
<h5 id="roc-curve-and-area-under-the-curve-auc">ROC curve and Area Under
The Curve (AUC)</h5>
<figure>
<img src="./img/AUC.png" alt="Area under curve" />
<figcaption aria-hidden="true">Area under curve</figcaption>
</figure>
<h3 id="multiclass-classification">Multiclass classification</h3>
<h4 id="one-vs-rest">One-vs-Rest</h4>
<p>Turn multiclass into binary classification (eg: classes [green, blue,
red] -&gt; one-vs-rest: [green, rest[blue, red]])</p>
<ul>
<li>Classification based on voting</li>
<li>NumClass * (NumClass – 1)/2 classifiers to train</li>
</ul>
<h2 id="training-models">4. Training models</h2>
<h3 id="linear-regression">Linear regression</h3>
<ul>
<li><strong>Assumption</strong> (Inductive bias): There is a linear
relationship between the input features and the target.</li>
<li><span
class="math inline"><em>P</em><em>r</em><em>i</em><em>c</em><em>e</em> = <em>θ</em><sub>0</sub> + <em>B</em><em>e</em><em>d</em><em>r</em><em>o</em><em>o</em><em>m</em><em>s</em> * <em>θ</em><sub>1</sub></span>
<ul>
<li><span class="math inline"><em>θ</em><sub>0</sub></span> : intercept
Bias</li>
<li><span class="math inline"><em>θ</em><sub>1</sub></span> : slope
weight</li>
</ul></li>
<li>Goal: find optimal parameter that defines line that best fits the
data</li>
<li>The prediction is the weighted sum of the input features with an
additional bias</li>
<li><span
class="math inline"><em>ŷ</em> = <em>h</em><sub><em>θ</em></sub>(<em>x</em>) = <em>θ</em> ⋅ <em>x</em></span>
<ul>
<li><span
class="math inline"><em>ŷ</em> : <em>p</em><em>r</em><em>e</em><em>d</em><em>i</em><em>c</em><em>t</em><em>i</em><em>o</em><em>n</em></span></li>
<li>x: input features, extended with a “1” value (as bias)</li>
<li><span class="math inline"><em>θ</em></span>: model parameters</li>
</ul></li>
</ul>
<figure>
<img src="./img/linreg_vector.png" alt="Linear regression vector" />
<figcaption aria-hidden="true">Linear regression vector</figcaption>
</figure>
<h4 id="linear-regression-training">Linear regression training</h4>
<ul>
<li>Minimize some loss function that measures how good <span
class="math inline"><em>θ</em></span> is (Root Mean Square Error): <span
class="math inline">$RMSE = \sqrt{\sum_{i=1}^{n} \frac{(\hat{y}_{i} -
y_{i})^2}{n}}$</span></li>
<li>Multiple options to find <span class="math inline"><em>θ̂</em></span>
<ul>
<li>Direct “closed form” solution</li>
<li>Gradient descent
<ul>
<li>Batch</li>
<li>Mini-batch</li>
<li>Stochastic</li>
</ul></li>
</ul></li>
</ul>
<h4 id="direct-solution">Direct solution</h4>
<ul>
<li>Directly calculate the optimal parameter given a labelled dataset
(rare)</li>
<li>Like generating a trendline in excel</li>
<li><strong>Ordinary least squares</strong>
<ol type="1">
<li>Calculate the partial derivatives of the loss function with respect
to the parameters (<span
class="math inline"><em>Θ</em><sub>0</sub>, <em>Θ</em><sub>1</sub></span>)</li>
<li>Set derivatives to zero</li>
<li>Solve the set of equations</li>
</ol></li>
<li>Relies on matrix inversion: <span
class="math inline"><em>O</em>(<em>n</em><sup>3</sup>)</span></li>
</ul>
<h4 id="gradient-descent">Gradient descent</h4>
<dl>
<dt><strong>Gradient descent</strong></dt>
<dd>
It is a first-order iterative algorithm for finding a local minimum of a
differentiable multivariate function.
</dd>
</dl>
<ul>
<li>Generic optimization algorithm</li>
<li>Procedure:
<ol type="1">
<li>Calculate the partial derivatives of the Loss function with respect
to the parameters (<span
class="math inline"><em>Θ</em><sub>0</sub>, <em>Θ</em><sub>1</sub></span>)</li>
<li>Pick random values for (<span
class="math inline"><em>Θ</em><sub>0</sub>, <em>Θ</em><sub>1</sub></span>)
and evaluate the partial derivative function. These describe how the
loss function changes when you change (<span
class="math inline"><em>Θ</em><sub>0</sub>, <em>Θ</em><sub>1</sub></span>)</li>
<li>Adjust (<span
class="math inline"><em>Θ</em><sub>0</sub>, <em>Θ</em><sub>1</sub></span>)
in opposite directions of the gradient</li>
<li>Repeat until convergence</li>
</ol></li>
<li><strong>Learning rate</strong> determines step size</li>
</ul>
<blockquote>
<p>Convex vs non-convex optimization problem<br />
<strong>Convex</strong>: There is no <em>local minimum</em>, only one
<em>global minimum</em>. Gradient descent guaranteed to find minimum.
<strong>Non-convex</strong>: There are <em>local minima</em>, meaning
that model can easily get stuck on bad model.</p>
</blockquote>
<h5 id="batch-gradient-descent">Batch Gradient descent</h5>
<p>Combination of Batch and Stochastic</p>
<p>Calculate the gradient using all available training data before
performing a step</p>
<ul>
<li>✅ You use all data -&gt; great model</li>
<li>❌ Very slow for small datasets</li>
</ul>
<h5 id="stochastic-gradient-descent">Stochastic Gradient descent</h5>
<p>Loop over all training data, calculate the gradient and take a step
for each sample</p>
<ul>
<li>✅ Fast</li>
<li>✅ Only uses memory for single sample instead of entire dataset</li>
<li>❌ Gradient estimate will be noisy
<ul>
<li>unlikely to find optimal solution</li>
<li>randomness might help to escape local minima</li>
</ul></li>
</ul>
<h5 id="mini-batch-gradient-descent">Mini-batch Gradient descent</h5>
<p>Combination of Batch and Stochastic</p>
<h3 id="polynomial-regression">Polynomial regression</h3>
<p>If the relationship is not linear. (eg: temperature and sales of ice
cream. Hot days mean lots of sales, but too hot and eating ice cream
becomes inconvenient)</p>
<ul>
<li>Fit on non-linear model: <span
class="math inline"><em>y</em> = <em>f</em>(<em>x</em>) = <em>Θ</em><sub>0</sub> + <em>Θ</em><sub>1</sub><em>x</em> + <em>Θ</em><sub>2</sub><em>x</em><sup>2</sup></span></li>
<li>Find (<span class="math inline"><em>Θ</em><sub>0</sub></span>, <span
class="math inline"><em>Θ</em><sub>1</sub></span>, <span
class="math inline"><em>Θ</em><sub>2</sub></span>) with gradient
descent</li>
<li>Transform data to space where it can be solved with linear
model</li>
</ul>
<h3 id="model-regularization">Model regularization</h3>
<p>There are 3 kinds of <strong>generalization</strong> (make useful
predictions on unseen data) errors:</p>
<dl>
<dt><strong>Bias error</strong></dt>
<dd>
Mistakes because of wrong assumptions when designing the model =
<em>underfitting</em> (eg: assuming linear relations)
</dd>
<dt><strong>Variance error</strong></dt>
<dd>
Mistakes made because the model is very sensitive to small variations in
the input = <em>overfitting</em>
</dd>
<dt><strong>Irreducible error</strong></dt>
<dd>
Noise in the data
</dd>
</dl>
<p>How to regularize a model:</p>
<ul>
<li>Limit the models expressive power -&gt; less overfitting -&gt;
better generalization</li>
<li>Add regularization term that forces weights to be small</li>
<li><strong>Ridge regression</strong>: Tries to approach a loss of
0</li>
<li><strong>Lasso regression</strong>: Tries to reach a loss of 0 (-&gt;
<strong>Sparse model</strong>)</li>
<li><strong>Elastic net</strong>: Weighted sum of Ridge and LASSO</li>
</ul>
<h3 id="cross-validation">Cross validation</h3>
<p>If there is too little data to afford splitting into test/train/val,
you can reuse a lot of data using cross-validation</p>
<ol type="1">
<li>Split data into K equal folds (K=5 or K=10)</li>
<li>Train using folds 1-9</li>
<li>Test using fold 10</li>
<li>Repeat 10 times with other folds as test set</li>
<li>Report average accuracy and standard deviation</li>
</ol>
<ul>
<li>✅: All data will be used for evaluation</li>
<li>❌: Computationally expensive</li>
</ul>
<h3 id="hyper-parameter-optimization">Hyper parameter optimization</h3>
<p>Automated procedures to find good hyper parameters:</p>
<ul>
<li><strong>Grid search</strong>: From each hyper param, combine some
values.</li>
<li><strong>Random search</strong>: Randomize hyper prarams</li>
</ul>
<blockquote>
<p>⚠️ : Optimizing hyper params is also a form of
<em>overfitting</em>!</p>
</blockquote>
<p>Best practice:</p>
<ol type="1">
<li>Split off test set and do not touch it!</li>
<li>Develop your model, optimizing hyper parameters with Random search
in combination with cross-validation</li>
<li>Retrain your model on all training data using the best hyper
parameters</li>
<li>Evaluate model on test data</li>
</ol>
<h3 id="classification-1">Classification</h3>
<p>Why not use linear regression for classification?</p>
<p>Regression models predict an exact value. Gradient descent will
change wights to adjust to latest train data, introducing errors for
other data.</p>
<h4 id="logistic-and-softmax-regression">Logistic and Softmax
regression</h4>
<blockquote>
<p>⚠️ : Despite the name, Logistic and Softmax are not regression
models</p>
</blockquote>
<h5 id="logistic-regression">Logistic regression</h5>
<ul>
<li><span
class="math inline"><em>p̂</em> = <em>σ</em>(0.25*<em>X</em><sub>1</sub>+0.125*<em>X</em><sub>2</sub>−1)</span></li>
<li>in general: <span
class="math inline"><em>p̂</em> = <em>h</em><sub><em>θ</em></sub>(<em>x</em>) = <em>σ</em>(<em>x</em><sup><em>T</em></sup><em>θ</em>)</span></li>
<li>With <span class="math inline"><em>σ</em></span> = sigmoid function:
<span class="math inline">$\sigma(t) = \frac{1}{1 + \exp(-t)}$</span>
<ul>
<li>&gt;0.5 if input is positive</li>
<li>&lt;0.5 if input is negative</li>
</ul></li>
<li>p close to 0 or 1: data lies far from decision boundary</li>
<li>p close to 0.5: data close to decision boundary</li>
</ul>
<figure>
<img src="./img/sigmoid.png" alt="Sigmoid function" />
<figcaption aria-hidden="true">Sigmoid function</figcaption>
</figure>
<p>Training logistic regression</p>
<ol type="1">
<li>Goal: find optimal parameters <span
class="math inline"><em>θ̂</em></span> that defines line that best
separates the data.</li>
<li>Optimize a cost function</li>
<li>Use <strong>Log loss</strong></li>
<li>Train using gradient descent with partial derivatives</li>
</ol>
<h5 id="softmax-regression">Softmax regression</h5>
<ul>
<li>For if there is more than one class</li>
<li>Predict a probability for each class and normalize them to sum to
one using the Softmax</li>
<li>The model is then trained using gradient descent with the cross
entropy loss</li>
</ul>
<figure>
<img src="./img/softmax_regression.png"
alt="example prediction using Softmax" />
<figcaption aria-hidden="true">example prediction using
Softmax</figcaption>
</figure>
<h2 id="super-vector-machines-linear-classification">5. Super Vector
Machines: Linear classification</h2>
<ul>
<li>Objective: Learn discriminating function g(x) that separates samples
of two categories.</li>
<li>Inductive Bias: g(x) is linear in its features (line in 3D, plane in
3D)
<ul>
<li><span
class="math inline"><em>g</em>(<em>x</em>) = <em>w</em><sub>1</sub><em>x</em><sub>1</sub> + <em>w</em><sub>2</sub><em>x</em><sub>2</sub> + <em>b</em> = <em>w</em> ⋅ <em>x</em> + <em>b</em> = <em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em></span>
<ul>
<li>b = bias term</li>
<li>w = direction of discriminating function</li>
</ul></li>
<li>Decision rule:
<ul>
<li><span
class="math inline"><em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em> ≥ 0</span>
-&gt; Sample belongs to class 1</li>
<li><span
class="math inline"><em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em> &lt; 0</span>
-&gt; Sample belongs to class 2</li>
</ul></li>
</ul></li>
</ul>
<figure>
<img src="./img/linear_classification.png"
alt="Linear classification in 2D" />
<figcaption aria-hidden="true">Linear classification in 2D</figcaption>
</figure>
<h3 id="largest-margin-classifier">Largest margin classifier</h3>
<p>Update decision rule to include margin:</p>
<ul>
<li><span
class="math inline"><em>g</em>(<em>x</em>) = <em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em> ≥ 1</span>
-&gt; Sample belongs to class 1</li>
<li><span
class="math inline"><em>g</em>(<em>x</em>) = <em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em> ≤  − 1</span>
-&gt; Sample belongs to class 2</li>
<li>Width of street becomes: <span class="math inline">$\frac{2}{\lVert
w \lVert}$</span></li>
<li>Margin functions:
<ul>
<li><span
class="math inline"><em>g</em>(<em>x</em><sub>+</sub>) = <em>w</em><sup><em>T</em></sup><em>x</em><sub>+</sub> + <em>b</em> = 1</span></li>
<li><span
class="math inline"><em>g</em>(<em>x</em><sub>−</sub>) = <em>w</em><sup><em>T</em></sup><em>x</em><sub>−</sub> + <em>b</em> =  − 1</span></li>
</ul></li>
</ul>
<p>To maximize the street width, we need to minimize <span
class="math inline">∥<em>w</em>∥</span>. For math convenience, we will
minimize <span class="math inline">$\frac{1}{2} \lVert w \lVert
^{2}$</span></p>
<figure>
<img src="./img/lin_classification_margin.png"
alt="Linear classification with margins" />
<figcaption aria-hidden="true">Linear classification with
margins</figcaption>
</figure>
<h4
id="importance-of-standardization-before-margin-classification">Importance
of standardization before margin classification</h4>
<p>When not standardized, the distance calculation is dominated by the
feature with the largest scale, therefor it is necessary to
scale/standardize all values using the following formula: <span
class="math inline">$x_{0, scaled} = \frac{x_{0} -
\mu_{x_{0}}}{\sigma_{x_{0}}}$</span></p>
<h3 id="soft-margin-classification">Soft margin classification</h3>
<p>Allows for:</p>
<ul>
<li>misclassification (can deal with outliers)</li>
<li>items in the margin</li>
</ul>
<p>Tolerates some errors: <span
class="math inline"><em>ζ</em><sup>(<em>i</em>)</sup> ≥ 0</span></p>
<p><span
class="math inline"><em>g</em>(<em>x</em><sup><em>i</em></sup>) = <em>t</em><sup>(<em>i</em>)</sup>(<em>w</em><sup><em>T</em></sup><em>x</em><sup>(<em>i</em>)</sup>+<em>b</em>) ≥ 1 − <em>ζ</em><sup>(<em>i</em>)</sup></span></p>
<figure>
<img src="./img/lin_class_margin_error.png"
alt="Linear classification with margin error" />
<figcaption aria-hidden="true">Linear classification with margin
error</figcaption>
</figure>
<h3 id="hard-margin-vs-soft-margin">Hard margin vs soft margin</h3>
<table>
<thead>
<tr class="header">
<th>Hard</th>
<th>Soft</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>small margin</td>
<td>bigger margin</td>
</tr>
<tr class="even">
<td>more accurate</td>
<td>less accurate</td>
</tr>
</tbody>
</table>
<h3 id="solving-svms">Solving SVM’s</h3>
<p><span class="math inline">$w = \sum_{i=1}^{n}
\alpha^{(i)}\,t^{(i)}\,x^{(i)} = \sum_{x^{(i)}}
\alpha^{(i)}\,t^{(i)}\,x^{(i)} + \sum_{x^{(i)}}
C\,t^{(i)}\,x^{(i)}$</span></p>
<p>Meaning: w = on the margin + in-margin or misclassified</p>
<ul>
<li><span
class="math inline"><em>α</em><sup>(<em>i</em>)</sup> = 0</span>: if
<span class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span> is on
the correct side of the boundary and outside margin</li>
<li><span
class="math inline"><em>α</em><sup>(<em>i</em>)</sup> = <em>C</em></span>:
if <span class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span> is
on the wrong side of the boundary or within the margin</li>
<li><span
class="math inline">0 &lt; <em>α</em><sup>(<em>i</em>)</sup> &lt; <em>C</em></span>:
if <span class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span> is
exactly on the margin</li>
</ul>
<h3 id="svm-for-non-linear-classification">SVM for non-linear
classification</h3>
<p>Example of non-linear data</p>
<figure>
<img src="./img/non-linear_data.png" alt="Non linear data graphs" />
<figcaption aria-hidden="true">Non linear data graphs</figcaption>
</figure>
<h4 id="the-kernel-trick">The kernel trick</h4>
<p>It is possible to project data to higher dimension, and solve using
linear classification.</p>
<blockquote>
<p>❗: Due to hidden constants, the complexity of the solver scales
badly with the number of features, so the generalized Lagrangian is no
longer that efficient.</p>
</blockquote>
<h2 id="decision-trees">6. Decision Trees</h2>
<p>Break down a decision on a sequence of smaller decisions, each based
on a single feature.</p>
<p>Example:</p>
<pre class="text"><code>X1 = Age
X2 = Sex
X3 = Cholesterol

Y = Patient has heart disease

-&gt; Age &gt; 60?
        ├ yes -&gt; Cholesterol &gt; 300?
        │                ├ yes  -&gt; Y = 1
        │                └ No   -&gt; Y = 0
        └ No -&gt; Sex = Male?
                         ├ yes  -&gt; ...
                         └ No   -&gt; Y = 0</code></pre>
<h3 id="decision-trees-for-classification-vs-regression">Decision trees
for classification vs regression</h3>
<table>
<colgroup>
<col style="width: 51%" />
<col style="width: 48%" />
</colgroup>
<thead>
<tr class="header">
<th>Classification</th>
<th>Regression</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Predict class members</td>
<td>Predict continuous value</td>
</tr>
<tr class="even">
<td>Leaves store class probabilities</td>
<td>Leaves store values</td>
</tr>
<tr class="odd">
<td>Internal nodes define a split of the remaining data points</td>
<td>All items in the same subspace have the same prediction</td>
</tr>
</tbody>
</table>
<h3 id="building-a-decision-tree">Building a decision tree</h3>
<ol type="1">
<li>Given a dataset with features X and labels Y, split data in two by
asking questions</li>
<li>Question = compare one feature with a threshold</li>
<li>Iteratively split two subsets again by asking another question</li>
<li>Repeat until no data left or tree has certain hight</li>
</ol>
<h3 id="cart-classification-and-regression-trees-algorithm">CART
(classification and regression trees) algorithm</h3>
<blockquote>
<p>❗: High probability for a practical question on the exam!</p>
</blockquote>
<ol type="1">
<li>Loop over all features</li>
<li>Pick a threshold</li>
<li>Split data points in two based on this threshold</li>
<li>Measure how good split is with a <strong>cost function</strong></li>
<li>Pick best split</li>
<li>Repeat until split is empty or until tree height is reached</li>
</ol>
<p>Cost function: <span class="math inline">$J(k, t_{k}) =
\frac{m_{left}}{m}\,G_{left} +
\frac{m_{right}}{m}\,G_{right}$</span></p>
<ul>
<li>G = cost (Gini impurity)</li>
<li>k = different classes</li>
<li><span class="math inline"><em>t</em><sub><em>k</em></sub></span> =
Node</li>
<li>m = total number of instances in a node before the split</li>
</ul>
<p>Gini impurity = <span
class="math inline">$1-\sum_{i=1}^{C}(p_{i})^{2}$</span></p>
<ul>
<li>C = number of classes</li>
<li><span class="math inline"><em>p</em><sub><em>i</em></sub></span> =
fraction of data points that belong to class C in the subset</li>
</ul>
<figure>
<img src="./img/CART_example.png" alt="Cart exercise example" />
<figcaption aria-hidden="true">Cart exercise example</figcaption>
</figure>
<h4 id="entropy">Entropy</h4>
<p>Instead of the <em>Gini impurity</em> we can also use Entropy</p>
<dl>
<dt><strong>Entropy</strong></dt>
<dd>
Average level of “information”, inherent to the variable’s possible
outcome
</dd>
</dl>
<p>In decision tree, entropy is zero if a particular node only contains
samples of a single class.</p>
<h3 id="regularization">Regularization</h3>
<p>Decision trees, too, can overfit. Therefor regularization is
important. It limits the freedom of:</p>
<ul>
<li>Min_samples_leaf</li>
<li>Min_weight_fraction_leaf</li>
<li>Max_height</li>
<li>Max_leaf_node</li>
</ul>
<h3 id="are-decision-trees-any-good">Are decision trees any good?</h3>
<ul>
<li>✅: Easy to understand</li>
<li>✅: Can model non-linear data</li>
<li>✅: Whitebox model (eg: useful for medical prediction to know what
influenced the diagnosis prediction)</li>
<li>✅: Easy to implement</li>
<li>✅: Allows to determine feature importance</li>
<li>✅: Supports multiple outputs</li>
<li>❌: Unstable (small change in data can drastically change
model)</li>
<li>❌: All decisions are made by orthogonal decision boundaries (only
straight lines perpendicular to feature axis)</li>
<li>❌: Relatively inaccurate</li>
<li>❌: Prone to overfitting</li>
</ul>
<h2 id="ensembles">7. Ensembles</h2>
<ul>
<li>Combine many weak models into one strong model.</li>
</ul>
<p>Procedure:</p>
<ol type="1">
<li>Make sure all models learn different things</li>
<li>Combine individual predictions</li>
</ol>
<p>Different techniques: Voting, Boosting and Stacking</p>
<blockquote>
<p>💡: Decision trees are a good candidate for ensembles, because they
can completely change if the data changes a bit. This is a good quality
for ensembles.</p>
</blockquote>
<h3 id="voting">Voting</h3>
<p>Train models on slightly different datasets and combine them using
average or voting</p>
<p>Methods to make different versions:</p>
<ul>
<li><strong>Bagging</strong>: sample from the dataset with
replacement</li>
<li><strong>Pasting</strong>: sample without replacement</li>
<li><strong>Random subspaces</strong>: use different subsets of the
features</li>
<li><strong>Random patches</strong>: select both random data points and
random feature subsets</li>
</ul>
<h3 id="boosting">Boosting</h3>
<p>Train different models that correct each other’s mistakes.</p>
<p>Unlike voting, training is sequential and can not be performed in
parallel.</p>
<h4 id="adaboost">Adaboost</h4>
<p>Each sequential model focuses more on the data points that had
incorrect predictions for previous models</p>
<ul>
<li>Two types of weights:
<ul>
<li><strong>Instance weight</strong> <span
class="math inline"><em>w</em><sup>(<em>i</em>)</sup></span>: Weight of
each sample, focuses on samples that were misclassified by the previous
models</li>
<li><strong>Predictor weight</strong> <span
class="math inline"><em>α</em><sup>(<em>i</em>)</sup></span>: Weight of
each model, used to calculate the final prediction</li>
</ul></li>
</ul>
<ol type="1">
<li>Initialize every weight to 1/m</li>
<li>Train the model (calculate its weighted error rate)</li>
<li>Calculate the predictor weight</li>
<li>Update the instance weights and normalize them</li>
<li>Repeat 2-4 until max number of predictors is reached or sufficient
performance</li>
<li>Calculate weighted prediction of the different models</li>
</ol>
<h4 id="gradient-boost">Gradient boost</h4>
<p>Each sequential model tries to predict the residual error of the
previous models.</p>
<ul>
<li>The next predictor tries to predict the <strong>residual
error</strong> made by previous predictors</li>
<li>Final output is the sum of the predictions of the individual
models</li>
</ul>
<h3 id="stacking">Stacking</h3>
<p>Train a model using predictions of other models as input. Meaning
that the model will combine predictions of other models.</p>
<ol type="1">
<li>Split train set in two</li>
<li>Train different models on the first part</li>
<li>Extract the predictions of the models for the second part =
<strong>Blending data set</strong></li>
<li>Train a model on the predictions of the first models</li>
</ol>
<h2 id="dimensionality-reduction">8. Dimensionality Reduction</h2>
<p>Every data point is a point in an n-dimensional space.</p>
<h3 id="curse-of-dimensionality">Curse of dimensionality</h3>
<p>As the amount of features (= dimensions) grows, the amount of data we
need to generalize grows exponentially.</p>
<ul>
<li><em>Data becomes sparser: instances are further from each other in
higher dimensions</em></li>
<li><em>Distance loses its meaning</em>: all points become equally far
away from a given point</li>
</ul>
<dl>
<dt><strong>Manifold hypothesis</strong></dt>
<dd>
The hypothesis dictates that data is actually lower dimensional than the
original input space. (Therefor, we should be able to reduce dimensions
and preserve accurate data)
</dd>
</dl>
<p>Approaches to dimensionality reduction:</p>
<ul>
<li><strong>Feature selection</strong>: Only keep the useful
features</li>
<li><strong>Feature projection</strong>: Apply transformation and
projections to derive new features</li>
</ul>
<h3 id="linear-dimensionality-reduction">Linear dimensionality
reduction</h3>
<p>Dimensionality reduction on linear data.</p>
<h4
id="principal-component-analysis-pca-geometric-interpretation">Principal
Component Analysis (PCA): geometric interpretation</h4>
<p>Procedure:</p>
<ol type="1">
<li>Minimize projection error is equal to maximizing the variance of the
projected data</li>
<li>Find direction of largest variance in data (eigenvector with largest
eigenvalue) (1st principal component)</li>
<li>Find direction of remaining largest variance, orthogonal to 1st
principal components (2nd principal component)</li>
<li>etc…</li>
</ol>
<figure>
<img src="./img/linear_dimension_reduction_PCA.png"
alt="Example: projection on purple line will lead to larger variation" />
<figcaption aria-hidden="true">Example: projection on purple line will
lead to larger variation</figcaption>
</figure>
<p><strong>How to find the optimal components?</strong></p>
<p>Calculate the normalized eigenvectors of the covariancematrix.
(matrix that displays the covariance between every pair of variables in
a dataset)</p>
<ol type="1">
<li>The eigenvector <code>v</code> of a matrix <code>A</code> are the
vectors <code>v</code> such that if you multiply <code>A</code> with
<code>v</code>, you get a multiple of <code>v</code></li>
<li>The vector still points in the same direction but has different
length</li>
<li>The scalar <code>λ</code> is called the eigenvalue that belongs to
<code>v</code>.</li>
</ol>
<p><strong>Covariance</strong>: <span class="math inline">$\sigma(x_{1},
x_{2}) = \frac{1}{m-1} \sum_{i=1}^{m}(x_{1}^{(i)} -
\overline{x_{1}})\,(x_{2}^{(i)} - \overline{x_{2}})$</span></p>
<p>Covariance Matrix: <span
class="math inline"><em>M</em> = <em>U</em> ⋅ ∑ ⋅ <em>V</em><sup>*</sup></span></p>
<ul>
<li>M = covariance matrix</li>
<li>U = orthogonal matrix representing the left singular vectors</li>
<li><span class="math inline">∑</span> = diagonal matrix with singular
values on the diagonal</li>
<li>V = columns of vector V contain <strong>unit vectors</strong>
corresponding with principal components</li>
</ul>
<h4 id="pca-steps">PCA steps</h4>
<ol type="1">
<li>Center-mean the dataset</li>
<li>Compute eigenvectors and the corresponding eigenvalues of the
covariance matrix (via SVD (Singular Value Decomposition))</li>
<li>Sort the eigenvectors by decreasing eigenvalues</li>
<li>Select the eigenvectors of the d largest eigenvalues</li>
<li>Form an <code>n x d</code> matrix <code>W</code> containing the
second eigenvectors as columns</li>
<li>Use eigenvector matrix to transform the samples onto the new
subspace</li>
</ol>
<figure>
<img src="./img/PCA_calculation.png" alt="PCA calculation" />
<figcaption aria-hidden="true">PCA calculation</figcaption>
</figure>
<figure>
<img src="./img/explained_variance_ratio.png"
alt="Explained variance ratio" />
<figcaption aria-hidden="true">Explained variance ratio</figcaption>
</figure>
<blockquote>
<p>❗: Make exercises on slides 25 and 26!</p>
</blockquote>
<h3 id="non-linear-dimensionality-reduction">Non-linear dimensionality
reduction</h3>
<p>One technique to transform data to linear data is to use
<strong>Linear Local Embedding</strong>. In this technique, you preserve
the original relative distance to near neighbours in the reduced space
(Reduced dimension).</p>
<figure>
<img src="./img/lin_local_embedding.png"
alt="Example of dim reduction using linear local embedding" />
<figcaption aria-hidden="true">Example of dim reduction using linear
local embedding</figcaption>
</figure>
<h2 id="unsupervised-learning">9. Unsupervised learning</h2>
<h3 id="distance-based-algo">Distance-based algo</h3>
<ul>
<li>Goal: Find a group of similar instances</li>
<li>Clustering algo assigns a cluster number to each sample</li>
<li><strong>Unsupervised</strong>: There is no ground truth to evaluate
the performance of the algorithm</li>
</ul>
<h4 id="k-means-clustering">K-means clustering</h4>
<p>Strategy:</p>
<ol type="1">
<li>You want to make <code>k</code> clusters (desired number of
classes)</li>
<li>Randomly select <code>k</code> points from the dataset as initial
cluster centroids <span
class="math inline"><em>C</em><sup>(1)</sup>, …<em>C</em><sup>(<em>k</em>)</sup></span></li>
<li>For each data point (<span
class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span>) in the
dataset, calculate its distance to each centroid. Assign the data point
to the nearest centroid: <span class="math inline">$b^{(i, j)} =
\begin{cases} 1 &amp; \text{if} \quad j = argmin \sum_{l=i}^{m}
(x_{l}^{(i)} - C_{l}^{(k)})^{2} \\ 0 \end{cases}$</span></li>
<li>Calculate new values for centroids: <span
class="math inline">$C_{new}^{(k)} = \frac{\sum_{i=1}^{m} b^{(i,k)}
x^{(i)}}{\sum_{i=1}^{m} b^{(i,k)}}$</span></li>
<li>Repeat 3-4 until convergence criteria are met</li>
</ol>
<blockquote>
<p>💡: Possible stop criteria: length of cluster center shift, #
iteration, …</p>
</blockquote>
<figure>
<img src="./img/k-mean.png" alt="Example progression of k-means" />
<figcaption aria-hidden="true">Example progression of
k-means</figcaption>
</figure>
<dl>
<dt><strong>Problem</strong></dt>
<dd>
If initial randomly selected centroids are bad, it may never find the
right clusters.
</dd>
<dt><strong>Solution</strong></dt>
<dd>
Restart the k-means algo x-times and pick the clustering with the lowest
<strong>inertia</strong> (= mean average distance of each point to its
closest centroid)
</dd>
</dl>
<ul>
<li>❌: cannot deal with difference in density between clusters
<ul>
<li><strong>Variance</strong> of features must be similar for all
clusters</li>
</ul></li>
</ul>
<h5 id="finding-the-right-number-of-clusters-inertia">Finding the right
number of clusters: inertia</h5>
<ol type="1">
<li><code>k</code> as hyperparam</li>
<li>run k-mean algo for different values of <code>k</code></li>
<li>Compare results, the lower <em>inertia</em>, the better</li>
</ol>
<p>inertia = <span class="math inline">$\frac{1}{m} \min || x^{(i)} -
C^{j} ||^{2}$</span></p>
<blockquote>
<p>⚠️: Too many clusters might lead to overfitting</p>
</blockquote>
<h5 id="silhouette-score">Silhouette score</h5>
<p>The <strong>silhouette score</strong> is a measure used to evaluate
the quality of clustering in a dataset. It provides an indication of how
well-separated the clusters are. This score ranges from -1 to 1,
where:</p>
<ul>
<li>close to +1: the sample is far away from neighboring clusters,
indicating good separation.</li>
<li>close to 0: the sample is close to the decision boundary between two
neighboring clusters.</li>
<li>close to -1: sample might have been assigned to the wrong
cluster.</li>
</ul>
<p><span class="math inline">$s^{(i)} = \frac{b^{(i)}-a^{(i)}}{\max
(a^{(i)},b^{(i)})}$</span></p>
<ul>
<li><span class="math inline"><em>a</em><sup>(<em>i</em>)</sup></span>:
mean distance from point to all other points in same cluster</li>
<li><span class="math inline"><em>b</em><sup>(<em>i</em>)</sup></span>:
mean distance from point to all instances in the next closest
cluster</li>
</ul>
<figure>
<img src="./img/silhouette_analysis.png"
alt="Example of silhouette analysis" />
<figcaption aria-hidden="true">Example of silhouette
analysis</figcaption>
</figure>
<h4 id="dbscan-clustering">DBScan clustering</h4>
<p><em>Density-Based Spatial Clustering of Applications with Noise</em>
is a clustering algorithm particularly effective at identifying clusters
of arbitrary shapes and handling noise in the data.</p>
<p><strong>Core instance</strong>: point with least <code>minPts</code>
within distance <span class="math inline"><em>ϵ</em></span></p>
<ol type="1">
<li>Pick a random point <code>p</code></li>
<li>init a new cluster</li>
<li>inspect <code>p</code>
<ul>
<li>if <code>p</code> is not core: consider as noise</li>
<li>if <code>p</code> is core:
<ul>
<li>add all points within distance <span
class="math inline"><em>ϵ</em></span> to cluster</li>
<li>recursively inspect all point in neighbourhood</li>
</ul></li>
</ul></li>
<li>Sample new unclustered point and repeat</li>
</ol>
<ul>
<li>✅: no need to manually select number of clusters</li>
<li>✅: find clusters of arbitrary shape</li>
<li>✅: works well when clusters are dense enough and well-separated by
low-density regions</li>
<li>❌: not deterministic</li>
<li>❌: uses distance metric (beware of scaling and <em>curse of dim
reduction</em>)</li>
<li>❌: difficult to choose right hyperparam</li>
</ul>
<h4 id="hierarchical-clustering">Hierarchical clustering</h4>
<p>Cluster data into a hierarchy of groups or clusters. It doesn’t
require specifying the number of clusters beforehand.</p>
<figure>
<img src="./img/hierarchical_clustering.png"
alt="Example hierarchical clustering" />
<figcaption aria-hidden="true">Example hierarchical
clustering</figcaption>
</figure>
<p>Hyperparams:</p>
<ul>
<li><strong>Affinity</strong>: distance metric between points</li>
<li><strong>Linkage function</strong>: distance between clusters
<ul>
<li>complete linkage: max distance between elements of different
clusters</li>
<li>single linkage: min distance between elements of different
clusters</li>
</ul></li>
</ul>
<h3
id="probability-density-based-density-estimation-gaussian-mixture-model">Probability-density
based (density estimation): Gaussian Mixture Model</h3>
<h4 id="generative-modelling">Generative modelling</h4>
<p>A ML technique used to create models that can generate new data
similar to a given dataset. This is commonly used for image or text
generation.</p>
<p><strong>Inductive bias</strong>: instances in dataset are samples
form <em>unknown statistical distribution</em> over a feature space:
<span
class="math inline"><em>x</em><sup>(<em>i</em>)</sup> ∼ <em>P</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub>(<em>x</em>)</span></p>
<p><strong>Task</strong>: learn from the data a <em>probability density
function</em> that matches this unknown distribution: <span
class="math inline"><em>P</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub>(<em>x</em>) ≈ <em>P</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub>(<em>x</em>)</span></p>
<h4 id="distribution-family">Distribution family</h4>
<p><strong>Inductive bias</strong>: data is sampled from a member of a
particular <em>distribution family</em>: <span
class="math inline"><em>P</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub>(<em>x</em>) = <em>P</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub>(<em>x</em>;<em>θ</em>)</span></p>
<p><strong>Task</strong>: find the “best” values for the parameter
vector <span class="math inline"><em>θ</em></span> which we denote as
<span class="math inline"><em>θ</em><sup>*</sup></span></p>
<figure>
<img src="./img/example_distribution_families.png"
alt="Example of distribution families" />
<figcaption aria-hidden="true">Example of distribution
families</figcaption>
</figure>
<h4 id="use-cases-for-generative-models">Use cases for generative
models</h4>
<ul>
<li><strong>Anomaly detection</strong>: if <span
class="math inline"><em>P</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub>(<em>x</em><sub><em>n</em><em>e</em><em>w</em></sub>;<em>θ</em>*)</span>
is lower than threshold -&gt; anomaly
<ul>
<li><figure>
<img src="./img/generative-anomaly-detection.png"
alt="Generative model anomaly detection" />
<figcaption aria-hidden="true">Generative model anomaly
detection</figcaption>
</figure></li>
</ul></li>
<li><strong>Classification</strong>: generate a new sample with the
features of the new data and add a feature that represents a class. Do
this for every class and then compare the probabilities. <span
class="math inline"><em>P</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub>(<em>x</em><sub>1, <em>n</em><em>e</em><em>w</em></sub>,<em>x</em><sub>2, <em>n</em><em>e</em><em>w</em></sub>,<em>y</em>=1,<em>θ</em>*) ≫ <em>P</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub>(<em>x</em><sub>1, <em>n</em><em>e</em><em>w</em></sub>,<em>x</em><sub>2, <em>n</em><em>e</em><em>w</em></sub>,<em>y</em>=2,<em>θ</em>*)</span>
<ul>
<li>Where <code>x1</code> and <code>x2</code> are features form the data
and <code>y</code> is the class</li>
</ul></li>
<li><strong>Generating new samples</strong>
<ul>
<li><figure>
<img src="./img/generative-sample-generation.png"
alt="Generative model to make new samples" />
<figcaption aria-hidden="true">Generative model to make new
samples</figcaption>
</figure></li>
</ul></li>
</ul>
<h4 id="how-to-sample-from-a-distribution">How to sample from a
distribution?</h4>
<ol type="1">
<li>Make inverse cumulative distribution function</li>
<li>Sample a random number in [0, 1[ (CDF(x) = <span
class="math inline"><em>x</em><sub><em>n</em><em>e</em><em>w</em></sub></span>)</li>
</ol>
<figure>
<img src="./img/sample_from_distrebution.png"
alt="Sample from distribution" />
<figcaption aria-hidden="true">Sample from distribution</figcaption>
</figure>
<h3 id="density-estimation-maximum-likelihood">Density estimation:
Maximum likelihood</h3>
<p><strong>Inductive bias</strong>: universal Gaussian distribution</p>
<p><strong>Task</strong>: find “best” values for <span
class="math inline"><em>θ</em> = [<em>μ</em>,<em>σ</em>]</span></p>
<h4 id="likelihood-function-to-find-best-distribution">Likelihood
function (to find best distribution)</h4>
<p>Every item <code>x</code> in our dataset is a sample of an unknown
distribution <span
class="math inline"><em>P</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub>(<em>x</em>)</span>.
We assume that this distribution is a member of a family of
distributions <span
class="math inline"><em>P</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub>(<em>x</em>;<em>θ</em>)</span>.
The best model should maximize the <strong>joint probability</strong> of
all items <span
class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span>.</p>
<p><span class="math display">$$P_{model}(x^{(1)},\ldots,x^{(m)};
\theta) = \prod_{i=1}^{m} P_{model}(x^{(i)};\theta) =
\mathcal{L}(\theta)$$</span></p>
<blockquote>
<p>❗: High probability for a practical question on the exam!</p>
</blockquote>
<h4 id="negative-loglikelihood-estimation">Negative Loglikelihood
estimation</h4>
<p>Often used to find the minimal loss function.</p>
<figure>
<img src="./img/negative-likelihood-estimation.png"
alt="Negative Loglikelihood estimation" />
<figcaption aria-hidden="true">Negative Loglikelihood
estimation</figcaption>
</figure>
<h3 id="soft-clustering-gaussian-mixture-distribution">Soft clustering:
Gaussian Mixture Distribution</h3>
<p>What if there are multiple groups in a dataset? For example a dataset
of dog pictures, these can be categorized in groups by dog-race.</p>
<h4 id="multimodel-distribution">Multimodel distribution</h4>
<p>Here, data is distributed as sum of multiple Gausssian distributions.
Note: this sum gets <em>downscaled</em> to maintain a surface of
<code>1</code>.</p>
<figure>
<img src="./img/multimodel-distr.png" alt="Mutimodal distribution" />
<figcaption aria-hidden="true">Mutimodal distribution</figcaption>
</figure>
<h4 id="gaussian-mixture-model-gmm">Gaussian Mixture Model (GMM)</h4>
<p><span class="math display">$$P_{model}(x;\theta) = \sum_{i=1}^{k}
\phi_{i} \mathcal{N}(x;\mu_{i},\sigma_{i})$$</span></p>
<p>In this formula, <span class="math inline"><em>ϕ</em></span> is the
factor with which to <em>rescale</em> the individual distributions to
maintain a valid density function (surface = 1).</p>
<figure>
<img src="./img/gmm-coefficients.png"
alt="GMM model: if a certain class is more represented than others, the mixture component will have a higher weight (right)" />
<figcaption aria-hidden="true">GMM model: if a certain class is more
represented than others, the mixture component will have a higher weight
(right)</figcaption>
</figure>
<h4 id="mle-of-gaussian-mixture-model">MLE of Gaussian Mixture
Model</h4>
<p>The MLE (Maximum Likelihood Estimation) of a GMM involves finding the
parameters (<span class="math inline"><em>μ</em>, <em>σ</em></span>)
that best fit the observed data under the assumption that the data comes
from a mixture of Gaussian distributions.</p>
<p>However, data under GMM doesn’t have a closed mathematical solution
so we can’t maximize the likelihood function with concepts like gradient
descent. Instead, we use our prior knowledge (inductive bias)</p>
<ol type="1">
<li>If we know which source or component generated each data point (x),
we can estimate the parameters of <span
class="math inline"><em>μ</em></span> and <span
class="math inline"><em>σ</em></span> by fitting a Gaussian
distribution.</li>
<li>If we know the means and variances, we can calculate the likelihood
of each data source to have generated x.</li>
</ol>
<h4 id="soft-labeling">Soft labeling</h4>
<p>We can label data by comparing the confidences (<span
class="math inline">𝒩(<em>x</em>;<em>μ</em><sub><em>i</em></sub>,<em>σ</em><sub><em>i</em></sub>)</span>)
of data to a certain GMM component.</p>
<figure>
<img src="./img/gmm-soft-labling.png" alt="Soft labeling" />
<figcaption aria-hidden="true">Soft labeling</figcaption>
</figure>
<h4 id="expection-maximization-algorithm-em">Expection-Maximization
algorithm (EM)</h4>
<ol type="1">
<li>Calculate soft-labels based on current distribution params
(expectation step)</li>
<li>Update distribution params (maximization step)</li>
<li>Repeat</li>
</ol>
<h4 id="soft-clustering">Soft Clustering</h4>
<p>In contrast to <em>k-means</em> clustering, where the model decides
which group a sample belongs to. We can quantify the likelihood a sample
belongs to a particular group by soft clustering the data.</p>
<p>Process:</p>
<ol type="1">
<li>Identify the number of clusters you want to split the data
into.</li>
<li>Define each cluster by generating it a Gaussian model.</li>
<li>For every sample, calculate the probability it belongs to a certain
cluster.</li>
<li>Recalculate the Gaussian models using the above probabilities.</li>
<li>Repeat until high set of probabilities.</li>
</ol>
<h3 id="multivariate-gmm">multivariate GMM</h3>
<h4 id="multivariate-unimodel-gaussian">Multivariate unimodel
Gaussian</h4>
<p>A multi-dimensional Gaussian distribution (multivariate) that has a
single peak or mode. It’s a type of multivariate Gaussian distribution
where the data tends to cluster around a single central point</p>
<figure>
<img src="./img/multivariate-uni.png"
alt="Example of multivariate unimodel Gaussian" />
<figcaption aria-hidden="true">Example of multivariate unimodel
Gaussian</figcaption>
</figure>
<p><span
class="math display"><em>p</em>(<em>x</em>) = <em>p</em>(<em>x</em><sub>1</sub>,…,<em>x</em><sub><em>n</em></sub>) = 𝒩(<em>x</em>;<em>μ</em>,<em>Σ</em>)</span></p>
<p>Where:</p>
<ul>
<li><span
class="math inline"><em>μ</em> = [<em>μ</em><sub><em>x</em></sub> <em>μ</em><sub><em>y</em></sub>]</span></li>
<li><span class="math inline">$\Sigma = \begin{pmatrix} \sigma_{x}^{2}
&amp;\rho\sigma_{x}\sigma_{y} \\ \rho\sigma_{x}\sigma_{y}
&amp;\sigma_{y}^{2} \end{pmatrix}$</span></li>
</ul>
<h4 id="multivariate-multimodel-gaussian">Multivariate multimodel
Gaussian</h4>
<p>A multi-dimensional Gaussian distribution (multivariate) that has
multiple peaks. For example, a dataset of dogs. Every dog in that
distribution has its own multivariate unimodel Gaussian distribution (eg
height x wight).</p>
<p><span class="math display">$$p(x) = \sum_{i=1}^{k} \phi_{i}
\mathcal{N}(x;\mu_{i},\Sigma_{i})$$</span></p>
<p>Where:</p>
<ul>
<li><span
class="math inline"><em>μ</em><sub><em>i</em></sub> = [<em>μ</em><sub><em>i</em><em>x</em></sub> <em>μ</em><sub><em>i</em><em>y</em></sub>]</span></li>
<li><span class="math inline">$\Sigma_{i} = \begin{pmatrix}
\sigma_{ix_{1}}^{2} &amp;\rho_{i}\sigma_{1x_{1}}\sigma_{1x_{2}} \\
\rho_{i}\sigma_{ix_{1}}\sigma_{ix_{2}} &amp;\sigma_{ix_{2}}^{2}
\end{pmatrix}$</span></li>
</ul>
<h4 id="mle-result-can-be-used-for-clustering">MLE result can be used
for clustering</h4>
<figure>
<img src="./img/MLE-clustering.png" alt="Example MLE for clustering" />
<figcaption aria-hidden="true">Example MLE for clustering</figcaption>
</figure>
<hr />
<h2 id="introduction-to-artificial-neural-networks-with-keras">10.
Introduction to Artificial Neural Networks with Keras</h2>
<h3 id="wave-1-perceptron">10.1 Wave 1: Perceptron</h3>
<dl>
<dt><strong>Perceptron</strong></dt>
<dd>
Single-layer NN that aims to solve binary classification problems.
</dd>
</dl>
<p><strong>Different components</strong>:</p>
<ul>
<li>Inputs</li>
<li>Weights</li>
<li>Weighted sum</li>
<li>Step function</li>
</ul>
<figure>
<img src="./img/perceptron.png" alt="perceptron" />
<figcaption aria-hidden="true">perceptron</figcaption>
</figure>
<p>Weights get updated using the following formula:</p>
<p><span
class="math display"><em>w</em><sub><em>i</em>, <em>j</em></sub><sup>(<em>n</em><em>e</em><em>x</em><em>t</em> <em>s</em><em>t</em><em>e</em><em>p</em>)</sup> = <em>w</em><sub><em>i</em>, <em>j</em></sub> + <em>η</em>(<em>y</em><sub><em>j</em></sub>−<em>ŷ</em><sub><em>j</em></sub>)<em>x</em><sub><em>i</em></sub></span></p>
<ul>
<li><span
class="math inline"><em>w</em><sub><em>i</em>, <em>j</em></sub><sup><em>n</em><em>e</em><em>x</em><em>t</em> <em>s</em><em>t</em><em>e</em><em>p</em></sup></span>:
Next weight value</li>
<li><span
class="math inline"><em>w</em><sub><em>i</em>, <em>j</em></sub></span>:
Old weight</li>
<li><span class="math inline"><em>η</em></span>: learning rate (constant
that determines the step size)</li>
<li><span
class="math inline"><em>y</em><sub><em>j</em></sub> − <em>ŷ</em><sub><em>j</em></sub></span>:
target - prediction (binary value)</li>
<li><span class="math inline"><em>x</em><sub><em>i</em></sub></span>:
input value</li>
</ul>
<p>The problem with the perceptron is that it is only capable of making
linear decision boundaries. (Not even able to solve an XOR).</p>
<h3 id="wave-2-distributed-representations">10.2 Wave 2: Distributed
representations</h3>
<p>The concept of <strong>connectionism</strong> got tracktion: Every
input should be represented by different features. Every feature should
be involved in the representation of the many inputs.</p>
<p><strong>Multi layered perceptrons</strong> (MLP) also got discovered.
These added layers allowed backpropagation to happen -&gt; non-linear
classification boundaries.</p>
<h3 id="wave-3-deep-learning">10.3 Wave 3: Deep learning</h3>
<p>Because of the better hardware, we were able to make deep neural
networks. And train them with more readily available data. A deep neural
network consists of multiple stacked layers of neurons, typically
trained using backpropagation (gradient descent).</p>
<figure>
<img src="./img/DNN.png" alt="Deep neural network" />
<figcaption aria-hidden="true">Deep neural network</figcaption>
</figure>
<p><strong>“Deep”</strong> refers to the fact that the model is able to
extract features from raw information instead of having humans extract
the manually.</p>
<p><strong>Logistic Regression</strong>: simple neural network:</p>
<figure>
<img src="./img/logistic-regression.png" alt="Logistic regression" />
<figcaption aria-hidden="true">Logistic regression</figcaption>
</figure>
<h4 id="activation-functions">Activation functions</h4>
<p>By default, each layer calculates a linear combination of the outputs
of the previous layer. By adding non-linear activations, we can make the
network behave in a non-linear way. (most common: ReLU)</p>
<figure>
<img src="./img/activation-functions.png" alt="Activation functions" />
<figcaption aria-hidden="true">Activation functions</figcaption>
</figure>
<h4 id="neural-network-training">Neural network training</h4>
<p>There is no closed solution to finding the minimal loss as the loss
function is <em>non-convex</em>. Instead we rely on <strong>gradient
descent</strong> which is not guaranteed to find the global minima.</p>
<p><strong>Backpropagation</strong> :Algorithm to calculate the gradient
with respect to the parameters of hidden layers.</p>
<h4
id="neural-networks-for-unsupervised-learning-dimensionality-reduction">Neural
networks for unsupervised learning / dimensionality reduction</h4>
<ul>
<li><strong>Autoencoder</strong>: model trained to reconstruct its own
input</li>
<li>Autoencoder has a <strong>bottleneck</strong> between the encoder
and decoder, effectively compressing the data.</li>
<li>After compressing, a <strong>decoder</strong> tries to reconstruct
it to original state</li>
<li>If there was no correlation between data this would not work.</li>
<li>If there is correlation, that structure can be learned</li>
</ul>
<p>Autoencoders are used for:</p>
<ul>
<li>Dimensionality reduction: discard decoder after training</li>
<li>Anomaly detection: reconstruction error is anomaly metric</li>
<li>Unsupervised pretraining</li>
</ul>
<p>Neural networks have many hyperparams that need to get
configured:</p>
<ul>
<li>Num of layers</li>
<li>Num of neurons/layer</li>
<li>Activation function</li>
<li>Learning rate &amp; batch size</li>
<li>Training algo and loss function</li>
<li>Initialization of params</li>
<li>Noise and data augmentation</li>
<li>Regularization</li>
</ul>
<hr />
<h2 id="training-deep-neural-networks">11. Training Deep Neural
Networks</h2>
<h3 id="vanishing-exploding-gradients">Vanishing / exploding
gradients</h3>
<p>By stacking multiple layers with sigmoid activations, the gradients
become very small for the first layers (<strong>Vanishing
gradients</strong>) and wont change much despite not being trained
good.</p>
<p>Situation:</p>
<ol type="1">
<li>Gradients are propagated backward through layers, they get
multiplied by weight matrices and activation functions’
derivatives.</li>
<li>Activation functions have regions where their derivatives become
very small (<span class="math inline"> ≈ 0</span>)</li>
<li>The deeper the gradient traverses in the network, the effect of
small gradients gets compounded until it effectively “vanishes”.</li>
</ol>
<h4 id="solution-1-weight-initialization">Solution 1: Weight
initialization</h4>
<p>The distribution of the initial weight factors can have a big impact
on the model training. It is best to give every weight an initial
±random value. A common strategy is to assign a weight a value from a
normal distribution.</p>
<h4 id="solution-2-activation-functions">Solution 2: Activation
functions</h4>
<blockquote>
<p>⚠️: Sigmoid activation functions are not the best choice because of
<strong>saturation</strong>. Saturation occurs because the activation
function squashes the input values to a number in the range of [0, 1].
Extreme input values (small or large) get a value close to 0 or 1 and
the gradient becomes very small.</p>
</blockquote>
<p>To avoid saturation, the use of the ReLU (Rectified Linear Units)
activation function, or one of its derivatives, is preferred. (ReLU
returns the value itself if it is bigger than 0)</p>
<h4 id="solution-3-batchnorm">Solution 3: Batchnorm</h4>
<ul>
<li>Additional layers after neural network layers</li>
<li>Normalizes the activations by subtracting a mean and dividing by the
standard deviation of the activations across the batch during
training</li>
<li>Keeps track of a moving average of these statistics to be used for
inference.</li>
<li>It is supposed to reduce “internal covariate shift”, but it is not
quite understood how or why this happens.</li>
</ul>
<h3 id="optimization-algorithms">Optimization algorithms</h3>
<p>There are some algorithms that can perform better than mini-batch
gradient.</p>
<ul>
<li><strong>Momentum</strong>: Use previous gradient to keep track of
the direction (faster and can escape local min/max)</li>
<li><strong>Adaptive learning rate</strong>: Algo that automatically
uses different learning rate for different parameter</li>
<li><strong>Second order method</strong>: Use second order partial
derivatives to estimate curvature around a point (expensive but gives
more insight in how far you are from optimum)</li>
</ul>
<figure>
<img src="./img/momentum.png"
alt="Example of algo with and without momentum" />
<figcaption aria-hidden="true">Example of algo with and without
momentum</figcaption>
</figure>
<h3 id="learning-rate-scheduling">Learning rate scheduling</h3>
<p>💡: Learning rate determines the size of the step the model takes
when searching for minimum. It is perhaps the most important hyper
parameter to tune. A good strategy is to start with a high learning rate
and decrease it overtime (decay).</p>
<h3 id="regularization-1">Regularization</h3>
<p>Regularization limits the freedom of the model. If you let the model
learn to much, it becomes prone to overfitting.</p>
<p>A couple regularization techniques:</p>
<ul>
<li>Use more training data</li>
<li>Use data augmentation and randomness</li>
<li>L1 and L2 regularization</li>
<li>Dropouts
<ul>
<li>Randomly set some nodes to zero during training</li>
<li>This increases robustness</li>
<li>Forces each neuron to be more independent</li>
</ul></li>
</ul>
<h3 id="normalization">Normalization</h3>
<p>Normalization is the process of scaling and transforming the input
features to a standard scale. The goal is to bring all features to a
similar scale, preventing certain features from dominating the learning
process simply because they have larger magnitudes.</p>
<p>Techniques:</p>
<ul>
<li>Min-max normalization: Scale to [0, 1]</li>
<li>Zero-center: subtract mean</li>
<li>Standardize: subtract mean, divide by standard deviation</li>
</ul>
<h3 id="hyper-parameter-tuning">Hyper parameter tuning</h3>
<p>This can be done manually (“grad student descent”) or automatically
(grid search, random search, …).</p>
<p>A good general approach:</p>
<ul>
<li>Use simple model without generalization</li>
<li>Sanity check the loss</li>
<li>Overfit on a small subset of the data
<ul>
<li>try different models</li>
<li>try different learning rates</li>
</ul></li>
<li>Train the most promising models on the full dataset for a few
epochs, try different optimizers</li>
<li>Apply regularization and train on the full dataset</li>
</ul>
<hr />
<h2 id="deep-computer-vision-using-convolutional-neural-networks">14.
Deep Computer vision using convolutional neural networks</h2>
<p>When working with input data with higher dimensions (like images),
traditional neural networks face some problems.</p>
<p>Lets take an image for example (200x200x3), first of all, this data
has to get reshaped to 1D (flatten).</p>
<ul>
<li><strong>Problem 1</strong>: The model needs to learn that certain
inputs are related</li>
<li><strong>Problem 2</strong>: 120000 input nodes + hidden layers make
for a very large neural network (consumes a lot of memory and is less
efficient to train)</li>
</ul>
<h3 id="convolutional-neural-networks">Convolutional neural
networks</h3>
<p>(CNN’s) are designed to solve these problems.</p>
<ul>
<li>Inputs are kept in 2D or even 3D (the <em>convolutional layers</em>
also keep these dimensions)</li>
<li>Not all neurons are connected to all neurons from previous layer.
Instead, every neuron is connected to some patches (e.g. 3x3) of neurons
from the previous layer</li>
<li><strong>Hierarchical structure</strong>
<ul>
<li>First layer (most nodes) focuses on small low-level features</li>
<li>Deeper layers assemble these into higher level features</li>
</ul></li>
<li><strong>Weights are reused</strong>
<ul>
<li>All neurons on the same layer use the same weights</li>
<li>These weights define a <strong>kernel</strong> (filter) that slide
over the input</li>
</ul>
By <strong>convolving</strong> the inputs with a suitable kernel, we
obtain a filtered version of the image or a <strong>feature
map</strong>. Depending on the kernel, it is also possible to learn
<strong>spatial</strong> features (like horizontal vs vertical lines or
certain color transitions). A single layers applies N filters and thus
generates N feature maps. The weights of these filters are determined
using gradient descent.</li>
</ul>
<figure>
<img src="./img/cnn-layers.png" alt="CNN model" />
<figcaption aria-hidden="true">CNN model</figcaption>
</figure>
<h4 id="convolutional-layer-output-size">Convolutional layer output
size</h4>
<p>The output size is determinded by the size of the kernel and the
<strong>stride</strong> (step between the places the filter is
applied)</p>
<p><span class="math display">$$n_{out} = \lfloor \frac{n_{in} + 2p -
k}{s} \rfloor + 1$$</span></p>
<ul>
<li><span
class="math inline"><em>n</em><sub><em>i</em><em>n</em></sub></span>:
num of input features</li>
<li><span
class="math inline"><em>n</em><sub><em>o</em><em>u</em><em>t</em></sub></span>:
num of output features</li>
<li><code>k</code>: conv kernel size</li>
<li><code>p</code>: conv padding size (sometimes <strong>zero
padding</strong> is applied so that every value of the input is taken
into account)</li>
<li><code>s</code>: conv stride size</li>
</ul>
<figure>
<img src="./img/convolution.png" alt="Convolution" />
<figcaption aria-hidden="true">Convolution</figcaption>
</figure>
<h4 id="convolutional-layer-number-of-parameters">Convolutional layer
number of parameters</h4>
<p>number of parameters = Number of kernels * kernel size + number of
kernels (bias term)</p>
<h3 id="pooling">Pooling</h3>
<p>After performing a convolution, the feature maps can still be quite
large. A solution to this is to subsampel the features from the feature
maps. In <strong>max pooling</strong> this is done by dividing the
feature maps into slices. For every slice, only the most dominant (max)
value is kept. The result is a feature map with with the same resolution
as the number of slices. It is also possible to pool in strides.</p>
<figure>
<img src="./img/pooling.png" alt="Pooling with 2x2 max-pool" />
<figcaption aria-hidden="true">Pooling with 2x2 max-pool</figcaption>
</figure>
<p><span class="math display">$$W_{out} = \frac{W - F}{S} +
1$$</span></p>
<ul>
<li><p><span
class="math inline"><em>W</em><sub><em>o</em><em>u</em><em>t</em></sub></span>
= output width/height</p></li>
<li><p><code>W</code> = input width/height</p></li>
<li><p><code>F</code> = pool size</p></li>
<li><p><code>S</code> = pool stride</p></li>
<li><p>✅: Reduce memory required</p></li>
<li><p>✅: Reduce computational cost</p></li>
<li><p>✅: Allow next layer to have larger receptive field</p></li>
<li><p>✅: Improve invariance to small translations of the
input</p></li>
<li><p>❌: Some loss of information</p></li>
</ul>
<h3 id="cnn-architecture">CNN architecture</h3>
<p>The final network is obtained by stacking</p>
<ul>
<li>Conv layers</li>
<li>Non-linear activations</li>
<li>(opt) batchnorm</li>
<li>Pooling</li>
</ul>
<p>To perform classification, the model is typically followed by:</p>
<ul>
<li>flatten</li>
<li>fully connected layer</li>
<li>activation function (softmax)</li>
</ul>
<figure>
<img src="./img/cnn-architecture.png" alt="CNN architecture" />
<figcaption aria-hidden="true">CNN architecture</figcaption>
</figure>
<h4 id="data-augmentation">Data augmentation</h4>
<p>For CNN, a common technique is to artificially increase the size of
the training set but generating variants of existing training instances
(rotate, flip, blur, crop, …). This often reduces overfitting.</p>
<hr />
<h2 id="processing-sequences-using-rnns-and-cnns">15. Processing
Sequences using RNNs and CNNs</h2>
<dl>
<dt><strong>Recurrent Neural Networks</strong></dt>
<dd>
NNs with neurons that have connections (weights) that point backwards.
This characteristic is desirable when working with sequential data like
time series prediction, speech recognition, NLP, video analysis, …
</dd>
</dl>
<p>The output of a step <code>t</code> is given together with the input
at step <code>t+1</code></p>
<p><span
class="math display"><em>y</em><sub><em>t</em></sub> = <em>ϕ</em>(<em>W</em><sub><em>x</em></sub><em>x</em><sub>(<em>t</em>)</sub>+<em>W</em><sub><em>y</em></sub><em>y</em><sub>(<em>t</em>−1)</sub>+<em>b</em>)</span></p>
<figure>
<img src="./img/rnn-output.png" alt="RNN output" />
<figcaption aria-hidden="true">RNN output</figcaption>
</figure>
<h3 id="rnn-architectures">RNN architectures</h3>
<p>There are a couple different RNN architectures</p>
<h4 id="seq2seq">Seq2seq</h4>
<p>For when both input and output are sequential. A use case of this
could be text-to-speech conversion.</p>
<figure>
<img src="./img/seq2seq.png" alt="Seq2seq" />
<figcaption aria-hidden="true">Seq2seq</figcaption>
</figure>
<h4 id="seq2vec">Seq2vec</h4>
<p>Input is sequential, but output is a fixed-size vector. A use case
for this could be sentimental analysis, or a speech-recognition tool
that has to understand only “hey google”.</p>
<figure>
<img src="./img/seq2vec.png" alt="Seq2vec" />
<figcaption aria-hidden="true">Seq2vec</figcaption>
</figure>
<h4 id="vec2seq">Vec2seq</h4>
<p>For when the input is a fixed-size vector, but the output needs to be
sequential. A use case for this could be image captioning; a model gets
an image as input and has to describe it.</p>
<figure>
<img src="./img/vec2seq.png" alt="Vec2seq" />
<figcaption aria-hidden="true">Vec2seq</figcaption>
</figure>
<h4 id="encoder-decoder">Encoder-decoder</h4>
<p>A combination of Seq2vec and Vec2seq, it serves as an alternative for
Seq2seq. A use case for this could be a language translation model the
translation system needs more than just one world, as the word could be
different depending on the context.</p>
<figure>
<img src="./img/encoder-decoder.png" alt="Encoder-decoder" />
<figcaption aria-hidden="true">Encoder-decoder</figcaption>
</figure>
<h3 id="backpropagation-through-time">Backpropagation through time</h3>
<p>To train an RNN, we unfold the network over time and train it like a
feed forward model.</p>
<p>Since the output of a neuron at a certain time step is a function of
all the inputs at previous time steps, a neuron in an RNN is also called
a <strong>memory cell</strong> (it preserves state).</p>
<figure>
<img src="./img/rnn-backpropagation.png" alt="RNN Backpropagation" />
<figcaption aria-hidden="true">RNN Backpropagation</figcaption>
</figure>
<p>RNNs are known to have some issues during trraining:</p>
<ul>
<li>RNNs are slow and unstable (especially for long sequences)</li>
<li><strong>Saturation</strong>: the same weights are used in every
step, we need to make sure that the output does not keep
increasing.</li>
</ul>
<h3 id="long-short-term-memory-lstms">Long-Short Term Memory
(LSTMs)</h3>
<p>RNNs output depends on the input of all previous timesteps, but for
long sequences, RNNs tend to forget the first inputs. That is why LSTM
models <em>explicitly define a memory slot</em>.</p>
<ul>
<li>Two memories:
<ul>
<li>Short term (preivious outputs)</li>
<li>Long term</li>
</ul></li>
<li>Model decides what to store in long term</li>
<li>output is based on input and both memories</li>
</ul>
<figure>
<img src="./img/LSTM.png" alt="LSTM" />
<figcaption aria-hidden="true">LSTM</figcaption>
</figure>
<blockquote>
<p>💡: On exam, not necessary to reproduce schema, but know how to
explain it!</p>
</blockquote>
<ul>
<li>Two states
<ul>
<li><span
class="math inline"><em>c</em><sub>(<em>t</em>)</sub> : <em>l</em><em>o</em><em>n</em><em>g</em><em>t</em><em>e</em><em>r</em><em>m</em></span></li>
<li><span
class="math inline"><em>h</em><sub>(<em>t</em>)</sub> : <em>s</em><em>h</em><em>o</em><em>r</em><em>t</em><em>t</em><em>e</em><em>r</em><em>m</em></span></li>
</ul></li>
<li>Three gate controllers
<ul>
<li><strong>Forget gate</strong>: controls which part of long term gets
erased</li>
<li><strong>Input gate</strong>: controls which part of
<code>g(t)</code> should get added to long term</li>
<li><strong>Output gate</strong>: Which part of long term should get
added to output</li>
</ul></li>
</ul>
<h3 id="gated-recurrent-units-gru">Gated Recurrent Units (GRU)</h3>
<p>A simplified version of LSTMs that work equally well.</p>
<ul>
<li>Single state <span
class="math inline"><em>h</em><sub>(<em>t</em>)</sub></span></li>
<li>Single gate controller layer which controls both forget gate and
input gate</li>
<li>No output gate</li>
<li>Different gate controller which decides what part of the state will
be used in current step</li>
</ul>
<figure>
<img src="./img/GRU.png" alt="GRU" />
<figcaption aria-hidden="true">GRU</figcaption>
</figure>
<hr />
<h2
id="natural-language-processing-with-rnns-and-attention-transformers">16.
Natural Language processing with RNNs and attention
(<strong>Transformers</strong>)</h2>
<h3 id="natural-language-preprocessing-nlp">Natural Language
Preprocessing (NLP)</h3>
<p>ML technique that uses text as input and/or output.</p>
<p>Different approaches:</p>
<ul>
<li>Traditional: feature extraction + ML model</li>
<li>RNN</li>
<li>Transformer based</li>
</ul>
<h4 id="basic-textual-feature-extraction">Basic textual feature
extraction</h4>
<ul>
<li>ML algos typically require numeric input data</li>
<li>N-grams: often, words are kept into sequences of N-words. For
example the bi-grams of the sentence “This is a sentence” becomes:
<code>["This is", "is a", "a sentence"]</code></li>
<li>Large vocab -&gt; sparse inputs</li>
<li>Text vectorization: convert each document to a vector of numerical
features</li>
</ul>
<p><span class="math display">$$w_{x,y} = tf_{x,y} .
\log(\frac{N}{df_{x}})$$</span></p>
<ul>
<li><code>w</code>: weight</li>
<li><code>tf</code>: freq of word</li>
<li><code>N</code>: number of documents</li>
<li><code>df</code>: number of documents where word occures</li>
</ul>
<h3 id="rnns-for-text-generation-seq2seq">RNNs for text generation
(seq2seq)</h3>
<ul>
<li>Pass a single input character at a time and predict the next
character in the sequence.</li>
<li>At inference time, we predict one char at a time passing the
previous output as input</li>
<li><em>Unsupervised pretraining</em>: The text encodings learned by the
model can be useful for other tasks like sentiment analysis.</li>
</ul>
<figure>
<img src="./img/rnn-text-gen.png" alt="RRN for text generation" />
<figcaption aria-hidden="true">RRN for text generation</figcaption>
</figure>
<h3 id="rnns-for-text-classification-seq2vec">RNNs for text
classification (seq2vec)</h3>
<p>Classification of text could be sentiment analysis. For the purpose
of classification, it’s best to pass the entire sentence as input. Every
word gets assigned a unique ID, and only the N most frequent words get
used to classify. The network typically has one <em>embedding layer</em>
that serves as a lookup table that converts every word to a learned
vector representation. The vector representation usually conveys how
close certain words are to each other.</p>
<h3 id="neural-machine-translation-encoder-decoder">Neural machine
translation (encoder-decoder)</h3>
<ul>
<li>Encoder receives input sentence, one word at a time</li>
<li>Decoder receives encoder state and correct previous output word</li>
</ul>
<figure>
<img src="./img/encoder-decoder-translation.png"
alt="Translation with encoder-decoder" />
<figcaption aria-hidden="true">Translation with
encoder-decoder</figcaption>
</figure>
<h4 id="bidirectional-rnn">Bidirectional RNN</h4>
<p>The representation of a word typically depends on both the previous
and the next word. That’s why a second RNN gets added that reads the
sentence backwards. Then, concatenate the two states of the encoder RNNs
before passing it to the decoder.</p>
<figure>
<img src="./img/bidirectional-rnn.png" alt="Bidirectinal RNN" />
<figcaption aria-hidden="true">Bidirectinal RNN</figcaption>
</figure>
<h4 id="beam-search">Beam search</h4>
<p>Keep track of the <code>k</code> most likely potential output
sentences. Try to extend all of these sentences with one additional
word.</p>
<h3 id="attention-mechanisms">Attention mechanisms</h3>
<p>The encoder-decoder architecture struggles with long sentences
because all the info needs to be encoded in the state. To solve this
issue, an attention mechanism gets used.</p>
<p>The attention mechanism keeps track of info related to all words in
the input sentence and allow the decoder to selectively attend to parts
of the input.</p>
<ul>
<li>We store the encoder’s output while preprocessing the input</li>
<li>At each timestep, the decoder receives a weighted sum of those
outputs as input.</li>
<li>Weights are predicted by an alignment model based on the decoder’s
current state and encoder outputs.</li>
</ul>
<h4 id="multi-head-attention">Multi-head attention</h4>
<p>For each word in the input sentence, the attention applies a linear
layer to obtain three tensors: key, value, query.</p>
<p>It then calculates the similarity between the word’s query vec with
all other key vecs. These similarities are used to calculate a weighted
average. This is repeated a couple of times to allow the model to
extract different types of relationships between input tokens.</p>
<h3 id="transformers">Transformers</h3>
<p>An architecture that does not make use of RNN or CNN, just fully
connected layers + attention. It does not suffer from vanishing gradient
and is better at capturing long-range patterns.</p>
<ol type="1">
<li>The sentence is split word by word</li>
<li>Each word is replaced by a learnable embedding</li>
<li>The sentence is then represented by a 3D tensor</li>
<li>Tensor is transformed while going through the model but the
dimensions remain the same</li>
</ol>
<p>The encoder transforms every input word into a high level
representation that captures the meaning of that word in this specific
sentence. While the decoder transforms the embedding of a translated
word into the embedding of the next translated word based on the
information extracted by the encoder.</p>
<figure>
<img src="./img/transformer.png" alt="Transformer" />
<figcaption aria-hidden="true">Transformer</figcaption>
</figure>
<h3 id="gpt">GPT</h3>
<ul>
<li>Generative pretrained transformer
<ul>
<li>Pretrained: predict the next token in the sentence</li>
<li>Finetuning: set of different task (summarization, text
classification or sentiment analysis)</li>
</ul></li>
<li>Previous models used supervised learning (e.g. translation) but
labeled datasets are expensive to collect</li>
</ul>
<h3 id="vision-transformer">Vision transformer</h3>
<p>Transformers can also be used for image classification
(e.g. ConvNet)</p>
<hr />
<h2 id="autoencoders-gans-and-diffusion-models">17. Autoencoders, GANs
and diffusion models</h2>
<h3 id="representation-learning-and-generative-modelling">Representation
learning and generative modelling</h3>
<p><strong>Representation learning</strong>:</p>
<ul>
<li>Learn good feature descriptions from data without manually defining
them</li>
<li>Autoencoders can be used to do it in an unsupervised way</li>
</ul>
<p><strong>Generative modelling</strong>:</p>
<ul>
<li>Can generate realistic “fake” samples</li>
<li>Can do stuff like colorization, image editing, future frame
prediction, …</li>
<li>Variational autoencoders</li>
<li>Generative Adversarial networks</li>
<li>Diffusion models</li>
</ul>
<h3 id="autoencoders">Autoencoders</h3>
<p>Also mentioned in chapter <a href="#wave-3-deep-learning">10.
Artificial NN’s dimensionality reduction</a></p>
<figure>
<img src="./img/autoencoder.png" alt="Autoencoder" />
<figcaption aria-hidden="true">Autoencoder</figcaption>
</figure>
<ul>
<li>The encoder transform data to a <strong>latent
representation</strong> (a compressed representation)</li>
<li>The decoder uses this representation to reconstruct input</li>
<li>Trained using gradient descent</li>
</ul>
<h4 id="manifold-hypothesis">Manifold hypothesis</h4>
<p>A hypothesis which explains why autoencoders work. It suggests that
high-dimensional data, such as real-world data, lies on or near a
lower-dimensional <strong>manifold</strong> within the high-dimensional
space. In other words, even though data might exist in a
high-dimensional space, it often occupies a lower-dimensional structure
or subspace.</p>
<p>By assuming that real-world data lies on a lower-dimensional
manifold, autoencoders learn to represent and generate data in a way
that captures the essential structure and variations within the
data.</p>
<h4 id="autoencoder-applications">Autoencoder applications</h4>
<ul>
<li><strong>Dimensionality reduction</strong>: sometimes autoencoders
(encoder) are better than PCA because they can also make non linear
mappings, in contrast to PCA</li>
<li><strong>Unsupervised pretraining</strong>: When the labeled data is
sparse:
<ul>
<li><ol type="1">
<li>Train autoencoder on labeled data</li>
</ol></li>
<li><ol start="2" type="1">
<li>Build a second model en use autoencoder of previous model as
classification layer in new model</li>
</ol></li>
<li><ol start="3" type="1">
<li>Train the second model</li>
</ol></li>
</ul></li>
<li><strong>Anomaly detection</strong>: Train autoencoder on good data
only, it will succeed in detecting anomalies because it wasn’t trained
on bad data.</li>
</ul>
<h4 id="autoencoder-variants">Autoencoder variants</h4>
<blockquote>
<p>💡: These are not all that common, so likely not an exam question</p>
</blockquote>
<ul>
<li><strong>Tying weights</strong>: reduce num of params by using the
same weights as the encoder and decoder</li>
<li><strong>Greedy layerwise training</strong>: first train autoencoder
with two layers, then train in between these two layers</li>
<li><strong>Sparse autoencoder</strong>: add regularization term during
training to encourage the learning of sparse representation</li>
</ul>
<h4 id="convolutional-autoencoder">Convolutional autoencoder</h4>
<p>For image inputs, its a good idea to add a CNN layer to the
autoencoder. The encoder reduces the spatial size, and increased the
number of feature maps. The decoder increases the spatial size, and
decreased the number of feature maps.</p>
<h4 id="transposed-convolution">Transposed convolution</h4>
<blockquote>
<p>💡: Also not that common , so likely not an exam question</p>
</blockquote>
<p>It’s an alternative for decoding, sometimes called “deconvolution”.
It upsamples the spatial dimensions of the input, equivalent to a normal
convolution with 0-padding and a flipped kernel.</p>
<h4 id="denoising-autoencoder">Denoising autoencoder</h4>
<p>Autoencoders can also be used to remove noise. Random noise is
applied to inputs, the model gets assessed based on the original input
(without the noise), so it learns to remove noise.</p>
<h4 id="variational-autoencoders">Variational autoencoders</h4>
<p>In contrast to normal autoencoders, the output is not deterministic,
rather, it is probabilistic. It is also generative, so it can generate
new samples.</p>
<ul>
<li>Latent vectors consists of two parts:
<ul>
<li>Mean coding (<span class="math inline"><em>μ</em></span>)</li>
<li>Standard deviation (<span
class="math inline"><em>σ</em></span>)</li>
</ul></li>
<li>During training, the encoder predicts both <span
class="math inline"><em>μ</em></span> and <span
class="math inline"><em>σ</em></span>.</li>
<li>Trained using two loss functions:
<ul>
<li>Reconstruction Loss (e.g. MSE)</li>
<li>Latent loss (to force the model to output codings that look like a
gaussian distribution)</li>
</ul></li>
<li>After training we can sample from N(0,1) and pass these sampled
latent codes to encoder to generate new samples.</li>
<li>VAEs simulate disentanglement of the latent factors.</li>
</ul>
<figure>
<img src="./img/variational-autoencoders.png"
alt="Variational autoencoder" />
<figcaption aria-hidden="true">Variational autoencoder</figcaption>
</figure>
<h3 id="generative-adversarial-models-gan">Generative adversarial models
(GAN)</h3>
<p>Two NNs get trained by competing with each other</p>
<ul>
<li><strong>Generator</strong>: generates a data sample based on
noise</li>
<li><strong>Discriminator</strong>: Tries to distinguish between real
and fake samples</li>
</ul>
<p>The generator never sees real samples, it is trained to generate
samples that will fool the discriminator.</p>
<figure>
<img src="./img/GAN.png" alt="GAN" />
<figcaption aria-hidden="true">GAN</figcaption>
</figure>
<h4 id="problems">Problems</h4>
<ul>
<li>Discriminator and generator compete with each other, this results in
a complex optimization problem</li>
<li>Very sensitive to hyper params</li>
<li><strong>Mode collapse</strong>: when generator becomes less
diverse</li>
</ul>
<h3 id="diffusion-models">Diffusion models</h3>
<ul>
<li><p>Training</p>
<ul>
<li>Keep adding noise to an image to generate increasingly noise
versions</li>
<li>Train denoiseing autoencoer to restore less noisy version from noisy
image</li>
</ul></li>
<li><p>Generating new image: start from random noise and apply all
denoiseing models</p></li>
<li><p>✅: Easy to train</p></li>
<li><p>✅: Can generate more diverse images</p></li>
<li><p>❌: Slow to generate new image</p></li>
</ul>
<dl>
<dt><strong>Latent diffusion models</strong></dt>
<dd>
Combine VAE and a diffusion model. The diffusion model operates on the
latent code of the VAE (generation is faster)
</dd>
</dl>
<h3 id="stable-diffusion">Stable diffusion</h3>
<p>Trained on images together with a human provided captions. The text
is preprocessed by a transformer based encoder. Stable diffusions is
used in image generation models like DALL-E.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Regression = predict a target value from sample’s
feature<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
            </div>
    </div>
  </div>
  <script src="https://vjs.zencdn.net/5.4.4/video.js"></script>

</body>
</html>
