<!doctype html>
<html >
<head>
    
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <!--[if lt IE 9]>
                <script src="http://css3-mediaqueries-js.googlecode.com/svn/trunk/css3-mediaqueries.js"></script>
        <![endif]-->
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta http-equiv="Content-Style-Type" content="text/css" />

    <!-- <link rel="stylesheet" type="text/css" href="template.css" /> -->
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/template.css" />

    <link href="https://vjs.zencdn.net/5.4.4/video-js.css" rel="stylesheet" />

    <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
    <!-- <script type='text/javascript' src='menu/js/jquery.cookie.js'></script> -->
    <!-- <script type='text/javascript' src='menu/js/jquery.hoverIntent.minified.js'></script> -->
    <!-- <script type='text/javascript' src='menu/js/jquery.dcjqaccordion.2.7.min.js'></script> -->

    <!-- <link href="menu/css/skins/blue.css" rel="stylesheet" type="text/css" /> -->
    <!-- <link href="menu/css/skins/graphite.css" rel="stylesheet" type="text/css" /> -->
    <!-- <link href="menu/css/skins/grey.css" rel="stylesheet" type="text/css" /> -->
  
    <!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
        
  
    <!-- <script src="script.js"></script> -->
  
    <!-- <script src="jquery.sticky-kit.js "></script> -->
    <script type='text/javascript' src='https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/js/jquery.cookie.js'></script>
    <script type='text/javascript' src='https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/js/jquery.hoverIntent.minified.js'></script>
    <script type='text/javascript' src='https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/js/jquery.dcjqaccordion.2.7.min.js'></script>

    <link href="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/css/skins/blue.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/css/skins/graphite.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/css/skins/grey.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.jsdelivr.net/gh/ryangrose/easy-pandoc-templates@948e28e5/css/elegant_bootstrap.css" rel="stylesheet" type="text/css" />
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
    <script src="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/script.js"></script>
  
    <script src="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/jquery.sticky-kit.js"></script>
    <meta name="generator" content="pandoc" />
  <title>Machine Learning summary</title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>

    
    <div class="navbar navbar-static-top">
    <div class="navbar-inner">
      <div class="container">
        <span class="doc-title">Machine Learning summary</span>
        <ul class="nav pull-right doc-info">
                            </ul>
      </div>
    </div>
  </div>
    <div class="container">
    <div class="row">
            <div id="TOC" class="span3">
        <div class="well toc">

        <ul>
        <li><a href="#machine-learning-summary"
        id="toc-machine-learning-summary">Machine Learning summary</a>
        <ul>
        <li><a href="#the-ml-landscape" id="toc-the-ml-landscape">1. The
        ML landscape</a>
        <ul>
        <li><a href="#ml-types" id="toc-ml-types">ML types</a></li>
        <li><a href="#model-based-vs-instance-based-learning"
        id="toc-model-based-vs-instance-based-learning">Model based vs
        instance based learning</a></li>
        <li><a href="#training-a-model"
        id="toc-training-a-model">Training a model</a></li>
        <li><a href="#ml-workflow" id="toc-ml-workflow">ML
        workflow</a></li>
        <li><a href="#problems-with-bad-datasets"
        id="toc-problems-with-bad-datasets">Problems with bad
        datasets</a></li>
        </ul></li>
        <li><a href="#end-to-end-ml-project"
        id="toc-end-to-end-ml-project">2. End-to-end ML project</a>
        <ul>
        <li><a href="#workflow" id="toc-workflow">Workflow</a></li>
        <li><a href="#exploratory-data-analysis-eda"
        id="toc-exploratory-data-analysis-eda">Exploratory Data Analysis
        (EDA)</a></li>
        </ul></li>
        <li><a href="#classification" id="toc-classification">3.
        Classification</a>
        <ul>
        <li><a href="#performance-metrics"
        id="toc-performance-metrics">Performance Metrics</a></li>
        <li><a href="#binary-classification"
        id="toc-binary-classification">Binary Classification</a></li>
        <li><a href="#multiclass-classification"
        id="toc-multiclass-classification">Multiclass
        classification</a></li>
        </ul></li>
        <li><a href="#training-models" id="toc-training-models">4.
        Training models</a>
        <ul>
        <li><a href="#linear-regression"
        id="toc-linear-regression">Linear regression</a></li>
        <li><a href="#polynomial-regression"
        id="toc-polynomial-regression">Polynomial regression</a></li>
        <li><a href="#model-regularization"
        id="toc-model-regularization">Model regularization</a></li>
        <li><a href="#cross-validation" id="toc-cross-validation">Cross
        validation</a></li>
        <li><a href="#hyper-parameter-optimization"
        id="toc-hyper-parameter-optimization">Hyper parameter
        optimization</a></li>
        <li><a href="#classification-1"
        id="toc-classification-1">Classification</a></li>
        </ul></li>
        <li><a href="#super-vector-machines-linear-classification"
        id="toc-super-vector-machines-linear-classification">5. Super
        Vector Machines: Linear classification</a>
        <ul>
        <li><a href="#largest-margin-classifier"
        id="toc-largest-margin-classifier">Largest margin
        classifier</a></li>
        <li><a href="#soft-margin-classification"
        id="toc-soft-margin-classification">Soft margin
        classification</a></li>
        <li><a href="#hard-margin-vs-soft-margin"
        id="toc-hard-margin-vs-soft-margin">Hard margin vs soft
        margin</a></li>
        <li><a href="#solving-svms" id="toc-solving-svms">Solving
        SVM’s</a></li>
        <li><a href="#svm-for-non-linear-classification"
        id="toc-svm-for-non-linear-classification">SVM for non-linear
        classification</a></li>
        </ul></li>
        <li><a href="#decision-trees" id="toc-decision-trees">6.
        Decision Trees</a>
        <ul>
        <li><a href="#decision-trees-for-classification-vs-regression"
        id="toc-decision-trees-for-classification-vs-regression">Decision
        trees for classification vs regression</a></li>
        <li><a href="#building-a-decision-tree"
        id="toc-building-a-decision-tree">Building a decision
        tree</a></li>
        <li><a
        href="#cart-classification-and-regression-trees-algorithm"
        id="toc-cart-classification-and-regression-trees-algorithm">CART
        (classification and regression trees) algorithm</a></li>
        <li><a href="#regularization"
        id="toc-regularization">Regularization</a></li>
        <li><a href="#are-decision-trees-any-good"
        id="toc-are-decision-trees-any-good">Are decision trees any
        good?</a></li>
        </ul></li>
        <li><a href="#ensembles" id="toc-ensembles">7. Ensembles</a>
        <ul>
        <li><a href="#voting" id="toc-voting">Voting</a></li>
        <li><a href="#boosting" id="toc-boosting">Boosting</a></li>
        <li><a href="#stacking" id="toc-stacking">Stacking</a></li>
        </ul></li>
        </ul></li>
        </ul>

        </div>
      </div>
            <div class="span9">

      
      <h1 id="machine-learning-summary">Machine Learning summary</h1>
<h2 id="the-ml-landscape">1. The ML landscape</h2>
<h3 id="ml-types">ML types</h3>
<table>
<colgroup>
<col style="width: 41%" />
<col style="width: 32%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th>Supervised</th>
<th>Unsupervised</th>
<th>Reinforcement</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Labeled data</td>
<td>No labels</td>
<td>Decision process</td>
</tr>
<tr class="even">
<td>Direct feedback</td>
<td>No feedback</td>
<td>Reward system</td>
</tr>
<tr class="odd">
<td>Predict outcome/future</td>
<td>Find hidden structures in data</td>
<td>Learn series of actions</td>
</tr>
<tr class="even">
<td>eg: Classification or regression<a href="#fn1" class="footnote-ref"
id="fnref1" role="doc-noteref"><sup>1</sup></a></td>
<td>eg: Anomaly detection</td>
<td>eg: alphago</td>
</tr>
</tbody>
</table>
<h3 id="model-based-vs-instance-based-learning">Model based vs instance
based learning</h3>
<table>
<colgroup>
<col style="width: 45%" />
<col style="width: 54%" />
</colgroup>
<thead>
<tr class="header">
<th>Model-based</th>
<th>Instance-based</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Evaluate a mathematical function on the unseen instance</td>
<td>Measure similarities between unseen instance and training
instance</td>
</tr>
</tbody>
</table>
<h3 id="training-a-model">Training a model</h3>
<ol type="1">
<li>Choose a parameterized model family (<span class="math inline">life
satisfaction = <em>θ</em><sub>0</sub> + <em>θ</em><sub>1</sub> ⋅ GDP_per_capita</span>)</li>
<li>Find parameter values that maximize a fitness function or minimize a
cost function</li>
</ol>
<dl>
<dt><strong>No free lunch</strong></dt>
<dd>
There is no model that is guaranteed to work better, a lot of testing
must happen to choose and fine-tune the model.
</dd>
</dl>
<h4 id="testing-and-validation">Testing and validation</h4>
<figure>
<img src="./img/testing_train_test.png"
alt="train using train and test data" />
<figcaption aria-hidden="true">train using train and test
data</figcaption>
</figure>
<figure>
<img src="./img/testing_train_test_val_png"
alt="train using train, test and validation data" />
<figcaption aria-hidden="true">train using train, test and validation
data</figcaption>
</figure>
<h4 id="overfitting">Overfitting</h4>
<p>Model doesn’t generalize enough. It learns your specific training
data but underperforms on new data.</p>
<p>Possible cures:</p>
<ul>
<li>Use a bigger dataset</li>
<li>Simplify model</li>
<li>Reduce the noice in the dataset</li>
</ul>
<h4 id="underfitting">Underfitting</h4>
<p>When the performance is even bad on the training data</p>
<p>Possible cures:</p>
<ul>
<li>Increase number of parameters</li>
<li>Add more features</li>
<li>Reduce the regularization parameters</li>
</ul>
<h3 id="ml-workflow">ML workflow</h3>
<figure>
<img src="./img/ml_workflow.png" alt="Machine learning workflow" />
<figcaption aria-hidden="true">Machine learning workflow</figcaption>
</figure>
<h3 id="problems-with-bad-datasets">Problems with bad datasets</h3>
<p><strong>Sampling bias</strong>: Dataset can be non-representative if
it has an underrepresented classes.</p>
<p>Garbage in == Garbage out: Bad dataset is guaranteed to lead to a bad
(trained) model.</p>
<h2 id="end-to-end-ml-project">2. End-to-end ML project</h2>
<h3 id="workflow">Workflow</h3>
<figure>
<img src="./img/project_workflow.png" alt="Project workflow" />
<figcaption aria-hidden="true">Project workflow</figcaption>
</figure>
<h3 id="exploratory-data-analysis-eda">Exploratory Data Analysis
(EDA)</h3>
<ol type="1">
<li>Get an initial feel of the data</li>
<li>Visualize and gain insight</li>
<li>Prepare the data</li>
</ol>
<h4 id="what-to-do-with-missing-values">What to do with missing
values</h4>
<ul>
<li>Remove entry</li>
<li><strong>Imputation</strong>: replace by mean, median, 0, …</li>
</ul>
<h4 id="categorical-attributes">Categorical attributes</h4>
<p>Attributes that can only take a limited number of values (eg: T-shirt
sizes)</p>
<p><strong>one-hot-encoding</strong>: Use on categorical variables to
transform them into a format the model can understand.</p>
<h4 id="features-scaling">Features scaling</h4>
<p>Some extremely big or small features may have an abnormally large
impact on the model. This can be solved by <em>rescaling</em> them using
the following techniques:</p>
<ul>
<li><strong>Normalization</strong> (min-max scaling): <span
class="math inline">$x_{norm} = \frac{x - min(x)}{max(x) -
min(x)}$</span></li>
<li><strong>Standardization</strong>: <span
class="math inline">$x_{stand} = \frac{x -
mean(x)}{standarddeviation(x)}$</span></li>
</ul>
<h2 id="classification">3. Classification</h2>
<p>Classification always happens by <em>supervised learning</em>
models</p>
<h3 id="performance-metrics">Performance Metrics</h3>
<h4 id="accuracy">Accuracy</h4>
<dl>
<dt><strong>Accuracy</strong></dt>
<dd>
The percentage of predicted labels that corresponds with the ground
truth label.
</dd>
</dl>
<p><span class="math inline">$Accuracy = \frac{TP +
TN}{Total}$</span></p>
<h4 id="confusion-matrix">Confusion Matrix</h4>
<p>Columns are predicted labels, rows are true labels</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Automobile</th>
<th>No Automobile</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Automobile</td>
<td>True Positive</td>
<td>False Negative</td>
</tr>
<tr class="even">
<td>No Automobile</td>
<td>False Positive</td>
<td>True Negative</td>
</tr>
</tbody>
</table>
<h4 id="precision-and-recall">Precision and recall</h4>
<dl>
<dt><strong>Precision</strong></dt>
<dd>
Accuracy of the positive predictions: <span
class="math inline">$precision = \frac{TP}{TP+FP}$</span>
</dd>
<dt>Recall</dt>
<dd>
How many of the actual positives are detected?: <span
class="math inline">$recall = \frac{TP}{TP+FN}$</span>
</dd>
</dl>
<p><strong>When do we want high precision?</strong></p>
<ul>
<li>When false positives are costly (eg: medical predictions, fraud
detection). You really don’t want falsely flag a condition.</li>
<li>In imbalanced datasets where the majority class is the negative
one.</li>
</ul>
<p><strong>When do we want high accuracy?</strong></p>
<ul>
<li>False negatives are costly (eg: cancer detection or nsfw
filters)</li>
<li>Information retrieval: recall helps ensure that all relevant
documents or information are retrieved</li>
</ul>
<h4 id="f1-score">F1 score</h4>
<p>It combines the precision and recall of a model into a single
metric.</p>
<p><span class="math inline">$F_{1} = 2 . \frac{precision \cdot
recall}{precision + recall}$</span></p>
<h3 id="binary-classification">Binary Classification</h3>
<dl>
<dt><strong>Binary</strong></dt>
<dd>
Only two classes.
</dd>
</dl>
<ul>
<li><strong>Decision boundary</strong>: hypersurface that partitions
underlying vector space into two sets, one for each class.</li>
<li><strong>Score/Class Probability</strong>: <span
class="math inline">$\hat{y}(x^{(i)}) = \begin{cases} +1 \quad
\text{if}\quad h_{\theta}(x^{(i)}) \geq T \\ -1 \quad \text{if}\quad
h_{\theta}(x^{(i)}) &lt; T \end{cases}$</span> (T = threshold as
hyperparam)</li>
</ul>
<h4 id="choosing-a-threshold">Choosing a threshold</h4>
<h5 id="precision-vs-recall-choosing-a-threshold">Precision vs Recall
(Choosing a threshold)</h5>
<figure>
<img src="./img/precision_vs_recall.png" alt="Precision vs recall" />
<figcaption aria-hidden="true">Precision vs recall</figcaption>
</figure>
<h5 id="roc-curve-and-area-under-the-curve-auc">ROC curve and Area Under
The Curve (AUC)</h5>
<figure>
<img src="./img/AUC.png" alt="Area under curve" />
<figcaption aria-hidden="true">Area under curve</figcaption>
</figure>
<h3 id="multiclass-classification">Multiclass classification</h3>
<h4 id="one-vs-rest">One-vs-Rest</h4>
<p>Turn multiclass into binary classification (eg: classes [green, blue,
red] -&gt; one-vs-rest: [green, rest[blue, red]])</p>
<ul>
<li>Classification based on voting</li>
<li>NumClass * (NumClass – 1)/2 classifiers to train</li>
</ul>
<h2 id="training-models">4. Training models</h2>
<h3 id="linear-regression">Linear regression</h3>
<ul>
<li><strong>Assumption</strong> (Inductive bias): There is a linear
relationship between the input features and the target.</li>
<li><span
class="math inline"><em>P</em><em>r</em><em>i</em><em>c</em><em>e</em> = <em>θ</em><sub>0</sub> + <em>B</em><em>e</em><em>d</em><em>r</em><em>o</em><em>o</em><em>m</em><em>s</em> * <em>θ</em><sub>1</sub></span>
<ul>
<li><span class="math inline"><em>θ</em><sub>0</sub></span> : intercept
Bias</li>
<li><span class="math inline"><em>θ</em><sub>1</sub></span> : slope
weight</li>
</ul></li>
<li>Goal: find optimal parameter that defines line that best fits the
data</li>
<li>The prediction is the weighted sum of the input features with an
additional bias</li>
<li><span
class="math inline"><em>ŷ</em> = <em>h</em><sub><em>θ</em></sub>(<em>x</em>) = <em>θ</em> ⋅ <em>x</em></span>
<ul>
<li><span
class="math inline"><em>ŷ</em> : <em>p</em><em>r</em><em>e</em><em>d</em><em>i</em><em>c</em><em>t</em><em>i</em><em>o</em><em>n</em></span></li>
<li>x: input features, extended with a “1” value (as bias)</li>
<li><span class="math inline"><em>θ</em></span>: model parameters</li>
</ul></li>
</ul>
<figure>
<img src="./img/linreg_vector.png" alt="Linear regression vector" />
<figcaption aria-hidden="true">Linear regression vector</figcaption>
</figure>
<h4 id="linear-regression-training">Linear regression training</h4>
<ul>
<li>Minimize some loss function that measures how good <span
class="math inline"><em>θ</em></span> is (Root Mean Square Error): <span
class="math inline">$RMSE = \sqrt{\sum_{i=1}^{n} \frac{(\hat{y}_{i} -
y_{i})^2}{n}}$</span></li>
<li>Multiple options to find <span class="math inline"><em>θ̂</em></span>
<ul>
<li>Direct “closed form” solution</li>
<li>Gradient descent
<ul>
<li>Batch</li>
<li>Mini-batch</li>
<li>Stochastic</li>
</ul></li>
</ul></li>
</ul>
<h4 id="direct-solution">Direct solution</h4>
<ul>
<li>Directly calculate the optimal parameter given a labelled dataset
(rare)</li>
<li>Like generating a trendline in excel</li>
<li><strong>Ordinary least squares</strong>
<ol type="1">
<li>Calculate the partial derivatives of the loss function with respect
to the parameters (<span
class="math inline"><em>Θ</em><sub>0</sub>, <em>Θ</em><sub>1</sub></span>)</li>
<li>Set derivatives to zero</li>
<li>Solve the set of equations</li>
</ol></li>
<li>Relies on matrix inversion: <span
class="math inline"><em>O</em>(<em>n</em><sup>3</sup>)</span></li>
</ul>
<h4 id="gradient-descent">Gradient descent</h4>
<dl>
<dt><strong>Gradient descent</strong></dt>
<dd>
It is a first-order iterative algorithm for finding a local minimum of a
differentiable multivariate function.
</dd>
</dl>
<ul>
<li>Generic optimization algorithm</li>
<li>Procedure:
<ol type="1">
<li>Calculate the partial derivatives of the Loss function with respect
to the parameters (<span
class="math inline"><em>Θ</em><sub>0</sub>, <em>Θ</em><sub>1</sub></span>)</li>
<li>Pick random values for (<span
class="math inline"><em>Θ</em><sub>0</sub>, <em>Θ</em><sub>1</sub></span>)
and evaluate the partial derivative function. These describe how the
loss function changes when you change (<span
class="math inline"><em>Θ</em><sub>0</sub>, <em>Θ</em><sub>1</sub></span>)</li>
<li>Adjust (<span
class="math inline"><em>Θ</em><sub>0</sub>, <em>Θ</em><sub>1</sub></span>)
in opposite directions of the gradient</li>
<li>Repeat until convergence</li>
</ol></li>
<li><strong>Learning rate</strong> determines step size</li>
</ul>
<blockquote>
<p>Convex vs non-convex optimization problem<br />
<strong>Convex</strong>: There is no <em>local minimum</em>, only one
<em>global minimum</em>. Gradient descent guaranteed to find
minimum.<br />
<strong>Non-convex</strong>: There are <em>local minima</em>, meaning
that model can easily get stuck on bad model.</p>
</blockquote>
<h5 id="batch-gradient-descent">Batch Gradient descent</h5>
<p>Combination of Batch and Stochastic</p>
<p>Calculate the gradient using all available training data before
performing a step</p>
<ul>
<li>✅ You use all data -&gt; great model</li>
<li>❌ Very slow for small datasets</li>
</ul>
<h5 id="stochastic-gradient-descent">Stochastic Gradient descent</h5>
<p>Loop over all training data, calculate the gradient and take a step
for each sample</p>
<ul>
<li>✅ Fast</li>
<li>✅ Only uses memory for single sample instead of entire dataset</li>
<li>❌ Gradient estimate will be noisy
<ul>
<li>unlikely to find optimal solution</li>
<li>randomness might help to escape local minima</li>
</ul></li>
</ul>
<h5 id="mini-batch-gradient-descent">Mini-batch Gradient descent</h5>
<p>Combination of Batch and Stochastic</p>
<h3 id="polynomial-regression">Polynomial regression</h3>
<p>If the relationship is not linear. (eg: temperature and sales of ice
cream. Hot days mean lots of sales, but too hot and eating ice cream
becomes inconvenient)</p>
<ul>
<li>Fit on non-linear model: <span
class="math inline"><em>y</em> = <em>f</em>(<em>x</em>) = <em>Θ</em><sub>0</sub> + <em>Θ</em><sub>1</sub><em>x</em> + <em>Θ</em><sub>2</sub><em>x</em><sup>2</sup></span></li>
<li>Find (<span class="math inline"><em>Θ</em><sub>0</sub></span>, <span
class="math inline"><em>Θ</em><sub>1</sub></span>, <span
class="math inline"><em>Θ</em><sub>2</sub></span>) with gradient
descent</li>
<li>Transform data to space where it can be solved with linear
model</li>
</ul>
<h3 id="model-regularization">Model regularization</h3>
<p>There are 3 kinds of <strong>generalization</strong> (make useful
predictions on unseen data) errors:</p>
<dl>
<dt><strong>Bias error</strong></dt>
<dd>
Mistakes because of wrong assumptions when designing the model =
<em>underfitting</em> (eg: assuming linear relations)
</dd>
<dt><strong>Variance error</strong></dt>
<dd>
Mistakes made because the model is very sensitive to small variations in
the input = <em>overfitting</em>
</dd>
<dt><strong>Irreducible error</strong></dt>
<dd>
Noise in the data
</dd>
</dl>
<p>How to regularize a model:</p>
<ul>
<li>Limit the models expressive power -&gt; less overfitting -&gt;
better generalization</li>
<li>Add regularization term that forces weights to be small</li>
<li><strong>Ridge regression</strong>: Tries to approach a loss of
0</li>
<li><strong>Lasso regression</strong>: Tries to reach a loss of 0 (-&gt;
<strong>Sparse model</strong>)</li>
<li><strong>Elastic net</strong>: Weighted sum of Ridge and LASSO</li>
</ul>
<h3 id="cross-validation">Cross validation</h3>
<p>If there is too little data to afford splitting into test/train/val,
you can reuse a lot of data using cross-validation</p>
<ol type="1">
<li>Split data into K equal folds (K=5 or K=10)</li>
<li>Train using folds 1-9</li>
<li>Test using fold 10</li>
<li>Repeat 10 times with other folds as test set</li>
<li>Report average accuracy and standard deviation</li>
</ol>
<ul>
<li>✅: All data will be used for evaluation</li>
<li>❌: Computationally expensive</li>
</ul>
<h3 id="hyper-parameter-optimization">Hyper parameter optimization</h3>
<p>Automated procedures to find good hyper parameters:</p>
<ul>
<li><strong>Grid search</strong>: From each hyper param, combine some
values.</li>
<li><strong>Random search</strong>: Randomize hyper prarams</li>
</ul>
<blockquote>
<p>⚠️ : Optimizing hyper params is also a form of
<em>overfitting</em>!</p>
</blockquote>
<p>Best practice:</p>
<ol type="1">
<li>Split off test set and do not touch it!</li>
<li>Develop your model, optimizing hyper parameters with Random search
in combination with cross-validation</li>
<li>Retrain your model on all training data using the best hyper
parameters</li>
<li>Evaluate model on test data</li>
</ol>
<h3 id="classification-1">Classification</h3>
<p>Why not use linear regression for classification?</p>
<p>Regression models predict an exact value. Gradient descent will
change wights to adjust to latest train data, introducing errors for
other data.</p>
<h4 id="logistic-and-softmax-regression">Logistic and Softmax
regression</h4>
<blockquote>
<p>⚠️ : Despite the name, Logistic and Softmax are not regression
models</p>
</blockquote>
<h5 id="logistic-regression">Logistic regression</h5>
<ul>
<li><span
class="math inline"><em>p̂</em> = <em>σ</em>(0.25*<em>X</em><sub>1</sub>+0.125*<em>X</em><sub>2</sub>−1)</span></li>
<li>in general: <span
class="math inline"><em>p̂</em> = <em>h</em><sub><em>θ</em></sub>(<em>x</em>) = <em>σ</em>(<em>x</em><sup><em>T</em></sup><em>θ</em>)</span></li>
<li>With <span class="math inline"><em>σ</em></span> = sigmoid function:
<span class="math inline">$\sigma(t) = \frac{1}{1 + \exp(-t)}$</span>
<ul>
<li>&gt;0.5 if input is positive</li>
<li>&lt;0.5 if input is negative</li>
</ul></li>
<li>p close to 0 or 1: data lies far from decision boundary</li>
<li>p close to 0.5: data close to decision boundary</li>
</ul>
<figure>
<img src="./img/sigmoid.png" alt="Sigmoid function" />
<figcaption aria-hidden="true">Sigmoid function</figcaption>
</figure>
<p>Training logistic regression</p>
<ol type="1">
<li>Goal: find optimal parameters <span
class="math inline"><em>θ̂</em></span> that defines line that best
separates the data.</li>
<li>Optimize a cost function</li>
<li>Use <strong>Log loss</strong></li>
<li>Train using gradient descent with partial derivatives</li>
</ol>
<h5 id="softmax-regression">Softmax regression</h5>
<ul>
<li>For if there is more than one class</li>
<li>Predict a probability for each class and normalize them to sum to
one using the Softmax</li>
<li>The model is then trained using gradient descent with the cross
entropy loss</li>
</ul>
<figure>
<img src="./img/softmax_regression.png"
alt="example prediction using Softmax" />
<figcaption aria-hidden="true">example prediction using
Softmax</figcaption>
</figure>
<h2 id="super-vector-machines-linear-classification">5. Super Vector
Machines: Linear classification</h2>
<ul>
<li>Objective: Learn discriminating function g(x) that separates samples
of two categories.</li>
<li>Inductive Bias: g(x) is linear in its features (line in 3D, plane in
3D)
<ul>
<li><span
class="math inline"><em>g</em>(<em>x</em>) = <em>w</em><sub>1</sub><em>x</em><sub>1</sub> + <em>w</em><sub>2</sub><em>x</em><sub>2</sub> + <em>b</em> = <em>w</em> ⋅ <em>x</em> + <em>b</em> = <em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em></span>
<ul>
<li>b = bias term</li>
<li>w = direction of discriminating function</li>
</ul></li>
<li>Decision rule:
<ul>
<li><span
class="math inline"><em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em> ≥ 0</span>
-&gt; Sample belongs to class 1</li>
<li><span
class="math inline"><em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em> &lt; 0</span>
-&gt; Sample belongs to class 2</li>
</ul></li>
</ul></li>
</ul>
<figure>
<img src="./img/linear_classification.png"
alt="Linear classification in 2D" />
<figcaption aria-hidden="true">Linear classification in 2D</figcaption>
</figure>
<h3 id="largest-margin-classifier">Largest margin classifier</h3>
<p>Update decision rule to include margin:</p>
<ul>
<li><span
class="math inline"><em>g</em>(<em>x</em>) = <em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em> ≥ 1</span>
-&gt; Sample belongs to class 1</li>
<li><span
class="math inline"><em>g</em>(<em>x</em>) = <em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em> ≤  − 1</span>
-&gt; Sample belongs to class 2</li>
<li>Width of street becomes: <span class="math inline">$\frac{2}{\lVert
w \lVert}$</span></li>
<li>Margin functions:
<ul>
<li><span
class="math inline"><em>g</em>(<em>x</em><sub>+</sub>) = <em>w</em><sup><em>T</em></sup><em>x</em><sub>+</sub> + <em>b</em> = 1</span></li>
<li><span
class="math inline"><em>g</em>(<em>x</em><sub>−</sub>) = <em>w</em><sup><em>T</em></sup><em>x</em><sub>−</sub> + <em>b</em> =  − 1</span></li>
</ul></li>
</ul>
<p>To maximize the street width, we need to minimize <span
class="math inline">∥<em>w</em>∥</span>. For math convenience, we will
minimize <span class="math inline">$\frac{1}{2} \lVert w \lVert
^{2}$</span></p>
<figure>
<img src="./img/lin_classification_margin.png"
alt="Linear classification with margins" />
<figcaption aria-hidden="true">Linear classification with
margins</figcaption>
</figure>
<h4
id="importance-of-standardization-before-margin-classification">Importance
of standardization before margin classification</h4>
<p>When not standardized, the distance calculation is dominated by the
feature with the largest scale, therefor it is necessary to
scale/standardize all values using the following formula: <span
class="math inline">$x_{0, scaled} = \frac{x_{0} -
\mu_{x_{0}}}{\sigma_{x_{0}}}$</span></p>
<h3 id="soft-margin-classification">Soft margin classification</h3>
<p>Allows for: - misclassification (can deal with outliers) - items in
the margin</p>
<p>Tolerates some errors: <span
class="math inline"><em>ζ</em><sup>(<em>i</em>)</sup> ≥ 0</span></p>
<p><span
class="math inline"><em>g</em>(<em>x</em><sup><em>i</em></sup>) = <em>t</em><sup>(<em>i</em>)</sup>(<em>w</em><sup><em>T</em></sup><em>x</em><sup>(<em>i</em>)</sup>+<em>b</em>) ≥ 1 − <em>ζ</em><sup>(<em>i</em>)</sup></span></p>
<figure>
<img src="./img/lin_class_margin_error.png"
alt="Linear classification with margin error" />
<figcaption aria-hidden="true">Linear classification with margin
error</figcaption>
</figure>
<h3 id="hard-margin-vs-soft-margin">Hard margin vs soft margin</h3>
<table>
<thead>
<tr class="header">
<th>Hard</th>
<th>Soft</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>small margin</td>
<td>bigger margin</td>
</tr>
<tr class="even">
<td>more accurate</td>
<td>less accurate</td>
</tr>
</tbody>
</table>
<h3 id="solving-svms">Solving SVM’s</h3>
<p><span class="math inline">$w = \sum_{i=1}^{n}
\alpha^{(i)}\,t^{(i)}\,x^{(i)} = \sum_{x^{(i)}}
\alpha^{(i)}\,t^{(i)}\,x^{(i)} + \sum_{x^{(i)}}
C\,t^{(i)}\,x^{(i)}$</span></p>
<p>Meaning: w = on the margin + in-margin or misclassified</p>
<ul>
<li><span
class="math inline"><em>α</em><sup>(<em>i</em>)</sup> = 0</span>: if
<span class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span> is on
the correct side of the boundary and outside margin</li>
<li><span
class="math inline"><em>α</em><sup>(<em>i</em>)</sup> = <em>C</em></span>:
if <span class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span> is
on the wrong side of the boundary or within the margin</li>
<li><span
class="math inline">0 &lt; <em>α</em><sup>(<em>i</em>)</sup> &lt; <em>C</em></span>:
if <span class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span> is
exactly on the margin</li>
</ul>
<h3 id="svm-for-non-linear-classification">SVM for non-linear
classification</h3>
<p>Example of non-linear data</p>
<figure>
<img src="./img/non-linear_data.png" alt="Non linear data graphs" />
<figcaption aria-hidden="true">Non linear data graphs</figcaption>
</figure>
<h4 id="the-kernel-trick">The kernel trick</h4>
<p>It is possible to project data to higher dimension, and solve using
linear classification.</p>
<blockquote>
<p>❗: Due to hidden constants, the complexity of the solver scales
badly with the number of features, so the generalized Lagrangian is no
longer that efficient.</p>
</blockquote>
<h2 id="decision-trees">6. Decision Trees</h2>
<p>Break down a decision on a sequence of smaller decisions, each based
on a single feature.</p>
<p>Example:</p>
<pre class="text"><code>X1 = Age
X2 = Sex
X3 = Cholesterol

Y = Patient has heart disease

-&gt; Age &gt; 60?
        ├ yes -&gt; Cholesterol &gt; 300?
        │                ├ yes  -&gt; Y = 1
        │                └ No   -&gt; Y = 0
        └ No -&gt; Sex = Male?
                         ├ yes  -&gt; ...
                         └ No   -&gt; Y = 0</code></pre>
<h3 id="decision-trees-for-classification-vs-regression">Decision trees
for classification vs regression</h3>
<table>
<colgroup>
<col style="width: 51%" />
<col style="width: 48%" />
</colgroup>
<thead>
<tr class="header">
<th>Classification</th>
<th>Regression</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Predict class members</td>
<td>Predict continuous value</td>
</tr>
<tr class="even">
<td>Leaves store class probabilities</td>
<td>Leaves store values</td>
</tr>
<tr class="odd">
<td>Internal nodes define a split of the remaining data points</td>
<td>All items in the smae subspace have the same prediction</td>
</tr>
</tbody>
</table>
<h3 id="building-a-decision-tree">Building a decision tree</h3>
<ol type="1">
<li>Given a dataset with features X and labels Y, split data in two by
asking questions</li>
<li>Question = compare one feature with a threshold</li>
<li>Iteratively split two subsets again by asking another question</li>
<li>Repeat until no data left or tree has certain hight</li>
</ol>
<h3 id="cart-classification-and-regression-trees-algorithm">CART
(classification and regression trees) algorithm</h3>
<blockquote>
<p>❗: High probability for a practical question on the exam!</p>
</blockquote>
<ol type="1">
<li>Loop over all features</li>
<li>Pick a threshold</li>
<li>Split data points in two based on this threshold</li>
<li>Measure hog good split is with a <strong>cost function</strong></li>
<li>Pick best split</li>
<li>Repeat until split is empty or until tree height is reached</li>
</ol>
<p>Cost function: <span class="math inline">$J(k, t_{k}) =
\frac{m_{left}}{m}\,G_{left} +
\frac{m_{right}}{m}\,G_{right}$</span></p>
<ul>
<li>G = cost (Gini impurity)</li>
<li>k = different classes</li>
<li><span class="math inline"><em>t</em><sub><em>k</em></sub></span> =
Node</li>
<li>m = total number of instances in a node before the split</li>
</ul>
<p>Gini impurity = <span
class="math inline">$1-\sum_{i=1}^{C}(p_{i})^{2}$</span></p>
<ul>
<li>C = number of classes</li>
<li><span class="math inline"><em>p</em><sub><em>i</em></sub></span> =
fraction of data points that belong to class C in the subset</li>
</ul>
<figure>
<img src="./img/CART_example.png" alt="Cart exercise example" />
<figcaption aria-hidden="true">Cart exercise example</figcaption>
</figure>
<h4 id="entropy">Entropy</h4>
<p>Instead of the <em>Gini impurity</em> we can also use Entropy</p>
<dl>
<dt><strong>Entropy</strong></dt>
<dd>
Average level of “information”, inherent to the variable’s possible
outcome
</dd>
</dl>
<p>In decision tree, entropy is zero if a particular node only contains
samples of a single class.</p>
<h3 id="regularization">Regularization</h3>
<p>Decision trees, too, can overfit. Therefor regularization is
important. It limits the freedom of:</p>
<ul>
<li>Min_samples_leaf</li>
<li>Min_weight_fraction_leaf</li>
<li>Max_height</li>
<li>Max_leaf_node</li>
</ul>
<h3 id="are-decision-trees-any-good">Are decision trees any good?</h3>
<ul>
<li>✅: Easy to understand</li>
<li>✅: Can model non-linear data</li>
<li>✅: Whitebox model (eg: useful for medical prediction to know what
influenced the diagnosis prediction)</li>
<li>✅: Easy to implement</li>
<li>✅: Allows to determine feature importance</li>
<li>✅: Supports multiple outputs</li>
<li>❌: Unstable (small change in data an drastically change model)</li>
<li>❌: All decisions are made by orthogonal decision boundaries (only
straight lines perpendicular to feature axis)</li>
<li>❌: Relatively inaccurate</li>
<li>❌: Prone to overfitting</li>
</ul>
<h2 id="ensembles">7. Ensembles</h2>
<ul>
<li>Combine many weak models into one strong model.</li>
</ul>
<p>Procedure:</p>
<ol type="1">
<li>Make sure all models learn different things</li>
<li>Combine individual predictions</li>
</ol>
<p>Different techniques: Voting, Boosting and Stacking</p>
<blockquote>
<p>💡: Decision trees are a good candidate for ensembles, because they
can completely change if the data change a bit. This is a good quality
for ensembles.</p>
</blockquote>
<h3 id="voting">Voting</h3>
<p>Train models on slightly different datasets and combine them using
average or voting</p>
<p>Methods to make different versions:</p>
<ul>
<li><strong>Bagging</strong>: sample from the dataset with
replacement</li>
<li><strong>Pasting</strong>: sample without replacement</li>
<li><strong>Random subspaces</strong>: use different subsets of the
features</li>
<li><strong>Random patches</strong>: select both random data points and
random feature subsets</li>
</ul>
<h3 id="boosting">Boosting</h3>
<p>Train different models that correct each other’s mistakes.</p>
<p>Unlike voting, training is sequential and can not be performed in
parallel.</p>
<h4 id="adaboost">Adaboost</h4>
<p>Each sequential model focuses more on the data points that had
incorrect predictions for previous models</p>
<ul>
<li>Two types of weights:
<ul>
<li><strong>Instance weight</strong> <span
class="math inline"><em>w</em><sup>(<em>i</em>)</sup></span>: Weight of
each sample, focuses on samples that were misclassified by the previous
models</li>
<li><strong>Predictor weight</strong> <span
class="math inline"><em>α</em><sup>(<em>i</em>)</sup></span>: Weight of
each model, used to calculate the final prediction</li>
</ul></li>
</ul>
<ol type="1">
<li>Initialize every weight to 1/m</li>
<li>Train the model (calculate its weighted error rate)</li>
<li>Calculate the predictor wight</li>
<li>Update the instance weights and normalize them</li>
<li>Repeat 2-4 until max number of predictors is reached or sufficient
performance</li>
<li>Calculate weighted prediction of the different models</li>
</ol>
<h4 id="gradient-boost">Gradient boost</h4>
<p>Each sequential model tries to predict the residual error of the
previous models.</p>
<ul>
<li>The next predictor tries to predict the <strong>residual
error</strong> made by previous predictors</li>
<li>Final output is the sum of the predictions of the individual
models</li>
</ul>
<h3 id="stacking">Stacking</h3>
<p>Train a model using predictions of other models as input. Meaning
that the model will combine predictions of other models.</p>
<ol type="1">
<li>Split train set in two</li>
<li>Train different models on the first part</li>
<li>Extract the predictions of the models for the second part =
<strong>Blending data set</strong></li>
<li>Train a model on the predictions of the first models</li>
</ol>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Regression = predict a target value from sample’s
feature<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
            </div>
    </div>
  </div>
  <script src="https://vjs.zencdn.net/5.4.4/video.js"></script>

</body>
</html>
